{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Multinomial Naive Bayes***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# IMPORTS\n",
    "# ============================\n",
    "import os\n",
    "import time\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import random\n",
    "import plotly.express as px\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import optuna.visualization as vis\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, label_binarize\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import (\n",
    "    f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    accuracy_score, log_loss, roc_auc_score, roc_curve, auc\n",
    ")\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from reporte_metricas import ReporteMetricas\n",
    "reporte = ReporteMetricas()\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# LOAD DATA\n",
    "# =======================\n",
    "file_path = \"../Saber_pro_sampled_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "#df.head(5000)\n",
    "X = df.drop(columns=[\"MOD_INGLES_DESEM\"])\n",
    "y = df[\"MOD_INGLES_DESEM\"]\n",
    "\n",
    "categorical_features = X.select_dtypes(include='object').columns.tolist()\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = pd.Series(le.fit_transform(y), index=y.index)\n",
    "\n",
    "class_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(\"Class mapping:\", class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ============================\n",
    "# # PATHS CONFIGURADOS PARA GUARDADO DE OBJETOS\n",
    "# # ============================\n",
    "model_filename = \"../Models/best_MNB_model.pkl\"\n",
    "metrics_filename = \"../Metrics/best_MNB_metrics.pkl\"\n",
    "study_filename = \"../Study/optuna_study_MNB.pkl\"\n",
    "fold_metrics_filename = \"../Metrics/MNB_folds_summary.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================\n",
    "# EVALUATION FUNCTION\n",
    "# ============================\n",
    "def evaluate_model(model, X_data, y_data):\n",
    "    y_pred = model.predict(X_data)\n",
    "    y_proba = model.predict_proba(X_data)\n",
    "    f1 = f1_score(y_data, y_pred, average='weighted')\n",
    "    acc = accuracy_score(y_data, y_pred)\n",
    "    loss = log_loss(y_data, y_proba)\n",
    "    auc = roc_auc_score(y_data, y_proba, multi_class='ovr', average='weighted')\n",
    "    report = classification_report(y_data, y_pred)\n",
    "    cm = confusion_matrix(y_data, y_pred)\n",
    "    return f1, acc, loss, auc, report, cm, y_proba, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pipeline(params, numeric_features, categorical_features):\n",
    "    # Preprocessing for numeric and categorical features\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', MinMaxScaler())\n",
    "        ]), numeric_features),\n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='Sin Dato')),\n",
    "            ('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "        ]), categorical_features)\n",
    "    ],\n",
    "        verbose_feature_names_out=False\n",
    "    )\n",
    "\n",
    "    model = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', MultinomialNB(**params))\n",
    "    ])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# FOLD METRIC SAVER\n",
    "# ============================\n",
    "def save_metrics_folds(folds_metrics: list, filename: str) -> pd.DataFrame:\n",
    "    df = pd.DataFrame(folds_metrics)\n",
    "    metric_cols = df.columns.drop('fold') if 'fold' in df.columns else df.columns\n",
    "    mean_row = df[metric_cols].mean().to_dict()\n",
    "    std_row = df[metric_cols].std().to_dict()\n",
    "    mean_row['fold'] = 'mean'\n",
    "    std_row['fold'] = 'std'\n",
    "    df_final = pd.concat([df, pd.DataFrame([mean_row, std_row])], ignore_index=True)\n",
    "    df_final.to_csv(filename, index=False)\n",
    "    print(f\"\\nüìÅ Fold metrics + summary saved to: {filename}\")\n",
    "    return df_final\n",
    "\n",
    "# ============================\n",
    "# CONFUSION MATRIX PLOTTER\n",
    "# ============================\n",
    "def plot_confusion_matrix(cm, labels, title):\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# OPTUNA INNER CV OPTIMIZATION\n",
    "# ============================\n",
    "def run_inner_optuna(X_inner, y_inner, numeric_features, categorical_features, n_trials=50): \n",
    "    def objective(trial):\n",
    "        params = {\"alpha\": trial.suggest_float(\"alpha\", 1e-3, 10.0, log=True)}\n",
    "        \n",
    "        model = build_pipeline(params, numeric_features, categorical_features)\n",
    "        skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "        scores = []\n",
    "        \n",
    "        for train_idx, val_idx in skf.split(X_inner, y_inner):\n",
    "            X_t, X_v = X_inner.iloc[train_idx], X_inner.iloc[val_idx]\n",
    "            y_t, y_v = y_inner.iloc[train_idx], y_inner.iloc[val_idx]\n",
    "            model.fit(X_t, y_t)\n",
    "            y_pred = model.predict(X_v)\n",
    "            scores.append(f1_score(y_v, y_pred, average='weighted'))\n",
    "        return np.mean(scores)\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\", pruner=MedianPruner())\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    return study.best_params, study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# NESTED CV\n",
    "# ============================\n",
    "def nested_cv(X: pd.DataFrame, y: pd.Series, numeric_features: list, categorical_features: list):\n",
    "    visualizations = {}\n",
    "    all_folds_metrics = []\n",
    "\n",
    "    if os.path.exists(model_filename) and os.path.exists(metrics_filename):\n",
    "        best_model = joblib.load(model_filename)\n",
    "        best_metrics = joblib.load(metrics_filename)\n",
    "        if \"best_fold\" in best_metrics:\n",
    "            study_dt = joblib.load(study_filename)['studies'][best_metrics['best_fold']]\n",
    "        else:\n",
    "            study_dt = None\n",
    "        if study_dt:\n",
    "            visualizations['optimization_history'] = vis.plot_optimization_history(study_dt)\n",
    "            visualizations['parallel_coordinate'] = vis.plot_parallel_coordinate(study_dt)\n",
    "            visualizations['param_importances'] = vis.plot_param_importances(study_dt)\n",
    "            f1_scores = [t.value for t in study_dt.trials]\n",
    "            fig = go.Figure()\n",
    "            fig.add_trace(go.Scatter(x=list(range(len(f1_scores))), y=f1_scores, mode='lines+markers', name='F1-score'))\n",
    "            fig.update_layout(title='F1-Score Evolution During Optuna Optimization', xaxis_title='Trial', yaxis_title='F1-Score', template='plotly_dark')\n",
    "            visualizations['f1_score_evolution'] = fig\n",
    "        return best_model, best_metrics, study_dt, visualizations\n",
    "\n",
    "    optuna_studies = []\n",
    "    outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    best_f1 = -np.inf\n",
    "    best_model = None\n",
    "    best_metrics = {}\n",
    "\n",
    "    for fold, (train_idx, test_idx) in tqdm(enumerate(outer_cv.split(X, y), 1), total=outer_cv.get_n_splits(), desc=\"Training folds\"):\n",
    "        X_train_fold, X_test_fold = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train_fold, y_test_fold = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "            X_train_fold, y_train_fold, test_size=0.2, stratify=y_train_fold, random_state=SEED\n",
    "        )\n",
    "\n",
    "        start_time = time.time()\n",
    "        best_params, study = run_inner_optuna(X_tr, y_tr, numeric_features, categorical_features)\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        optuna_studies.append(study)\n",
    "\n",
    "        model = build_pipeline(best_params, numeric_features, categorical_features)\n",
    "        model.fit(X_tr, y_tr)\n",
    "\n",
    "        f1_train, acc_train, loss_train, auc_train, report_train, cm_train, y_proba_train, y_train_pred = evaluate_model(model, X_tr, y_tr)\n",
    "        f1_val, acc_val, loss_val, auc_val, report_val, cm_val, y_proba_val, y_val_pred = evaluate_model(model, X_val, y_val)\n",
    "        f1_test, acc_test, loss_test, auc_test, report_test, cm_test, y_proba_test, y_test_pred = evaluate_model(model, X_test_fold, y_test_fold)\n",
    "\n",
    "        fold_metrics = {\n",
    "            \"fold\": fold,\n",
    "            \"f1_train\": f1_train,\n",
    "            \"accuracy_train\": acc_train,\n",
    "            \"log_loss_train\": loss_train,\n",
    "            \"auc_train\": auc_train,\n",
    "            \"f1_val\": f1_val,\n",
    "            \"accuracy_val\": acc_val,\n",
    "            \"log_loss_val\": loss_val,\n",
    "            \"auc_val\": auc_val,\n",
    "            \"f1_test\": f1_test,\n",
    "            \"accuracy_test\": acc_test,\n",
    "            \"log_loss_test\": loss_test,\n",
    "            \"auc_test\": auc_test,\n",
    "            \"optuna_time\": elapsed\n",
    "        }\n",
    "        all_folds_metrics.append(fold_metrics)\n",
    "\n",
    "        if f1_test > best_f1:\n",
    "            best_f1 = f1_test\n",
    "            best_model = model\n",
    "            best_metrics = {\n",
    "                **fold_metrics,\n",
    "                \"params\": best_params,\n",
    "                \"labels\": np.unique(y),\n",
    "                \"best_fold\": fold - 1,\n",
    "                \"X_train_fold\": X_tr,\n",
    "                \"y_train_fold\": y_tr,\n",
    "                \"y_train_pred\": y_train_pred,\n",
    "                \"y_train_true\": y_tr,\n",
    "                \"y_proba_train\": y_proba_train,\n",
    "                \"classification_report_train\": report_train,\n",
    "                \"confusion_matrix_train\": cm_train,\n",
    "                \"y_val_true\": y_val,\n",
    "                \"y_val_pred\": y_val_pred,\n",
    "                \"y_proba_val\": y_proba_val,\n",
    "                \"classification_report_val\": report_val,\n",
    "                \"confusion_matrix_val\": cm_val,\n",
    "                \"y_test_fold\": y_test_fold,\n",
    "                \"y_test_pred\": y_test_pred,\n",
    "                \"y_proba_test\": y_proba_test,\n",
    "                \"classification_report_test\": report_test,\n",
    "                \"confusion_matrix_test\": cm_test\n",
    "            }\n",
    "\n",
    "    joblib.dump(best_model, model_filename)\n",
    "    joblib.dump({\"studies\": optuna_studies}, study_filename)\n",
    "    joblib.dump(best_metrics, metrics_filename)\n",
    "    df_folds = save_metrics_folds(all_folds_metrics, fold_metrics_filename)\n",
    "\n",
    "    study_dt = optuna_studies[best_metrics['best_fold']]\n",
    "    visualizations['optimization_history'] = vis.plot_optimization_history(study_dt)\n",
    "    visualizations['parallel_coordinate'] = vis.plot_parallel_coordinate(study_dt)\n",
    "    visualizations['param_importances'] = vis.plot_param_importances(study_dt)\n",
    "\n",
    "    f1_scores = [t.value for t in study_dt.trials]\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=list(range(len(f1_scores))), y=f1_scores, mode='lines+markers', name='F1-score'))\n",
    "    fig.update_layout(title='F1-Score Evolution During Optuna Optimization', xaxis_title='Trial', yaxis_title='F1-Score', template='plotly_dark')\n",
    "    visualizations['f1_score_evolution'] = fig\n",
    "\n",
    "    return best_model, best_metrics, study_dt, visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, metrics, study, visualizations = nested_cv(X, y, numeric_features, categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics['classification_report_val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(metrics['confusion_matrix_val'], metrics['labels'], \"Validation Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics['classification_report_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(metrics['confusion_matrix_test'], metrics['labels'], \"Test Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# LIME EXPLANATION FOR NB\n",
    "# ============================\n",
    "\n",
    "X_transformed = model.named_steps[\"preprocessor\"].transform(metrics['X_train_fold'])\n",
    "\n",
    "def predict_fn(x):\n",
    "    return model.named_steps[\"classifier\"].predict_proba(x)\n",
    "\n",
    "feature_names = model.named_steps[\"preprocessor\"].get_feature_names_out()\n",
    "class_names = np.unique(metrics['y_train_fold']).astype(str)\n",
    "\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "    training_data=X_transformed,\n",
    "    feature_names=feature_names,\n",
    "    class_names=class_names,\n",
    "    mode='classification'\n",
    ")\n",
    "\n",
    "instance = X_transformed[10]\n",
    "\n",
    "exp = explainer.explain_instance(instance, predict_fn=predict_fn, labels=[0, 1, 2,3 ,4])\n",
    "\n",
    "exp.show_in_notebook(show_table=True)\n",
    "\n",
    "for class_label in exp.available_labels():\n",
    "    print(f\"--- Explanation for class {class_label} ---\")\n",
    "    fig = exp.as_pyplot_figure(label=class_label)\n",
    "    plt.title(f\"LIME - Class {class_label}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizations['optimization_history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizations['parallel_coordinate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizations['f1_score_evolution']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizations['param_importances']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reporte.save(metrics, model_name=\"Mult_NB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = reporte.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"Model == 'Mult_NB' and Type == 'train' and Class != 'global'\").iloc[:, 0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"Model == 'Mult_NB' and Type == 'val' and Class != 'global'\").iloc[:, 0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"Model == 'Mult_NB' and Type == 'test' and Class != 'global'\").iloc[:, 0:6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"Model == 'Mult_NB' and auc != '-'\")[[\"Model\", \"Type\", \"accuracy\", \"log_loss\", \"auc\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"Model == 'Mult_NB' and Type == 'val' and Class\t== 'global'\")[[\"Model\", \"Type\", \"accuracy\", \"log_loss\", \"auc\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_multiclass(y_true, y_proba, class_labels, title=\"AUC-ROC Curve (Multiclass)\"):\n",
    "    # Binarize true labels\n",
    "    y_bin = label_binarize(y_true, classes=class_labels)\n",
    "    n_classes = len(class_labels)\n",
    "\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], y_proba[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for i in range(n_classes):\n",
    "        plt.plot(fpr[i], tpr[i],\n",
    "                 label=f\"Class {class_labels[i]} (AUC = {roc_auc[i]:.3f})\")\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", label=\"Random\")\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_multiclass(\n",
    "    y_true=metrics[\"y_test_fold\"],\n",
    "    y_proba=metrics[\"y_proba_test\"],\n",
    "    class_labels=metrics[\"labels\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.named_steps['classifier'].get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(fold_metrics_filename)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
