{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***XG-Boost `tree_method='hist'`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "import optuna.visualization as vis\n",
    "\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, label_binarize, OrdinalEncoder, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    accuracy_score, log_loss, roc_auc_score, roc_curve, auc\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "import lime.lime_tabular\n",
    "import time\n",
    "\n",
    "from reporte_metricas import ReporteMetricas\n",
    "reporte = ReporteMetricas()\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =======================\n",
    "# LOAD DATA\n",
    "# =======================\n",
    "file_path = \"../Saber_pro_sampled_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "#df = df.head(1000)\n",
    "X = df.drop(columns=[\"MOD_INGLES_DESEM\"])\n",
    "y = df[\"MOD_INGLES_DESEM\"]\n",
    "\n",
    "categorical_features = X.select_dtypes(include='object').columns.tolist()\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = pd.Series(le.fit_transform(y), index=y.index)\n",
    "\n",
    "class_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(\"Class mapping:\", class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# FILE PATHS\n",
    "# ============================\n",
    "model_filename = \"../Models/best_xgb_model_hist.pkl\"\n",
    "study_filename = \"../Study/optuna_study_XGB_nested_hist.pkl\"\n",
    "metrics_filename = \"../Models/best_xgb_metrics_hist.pkl\"\n",
    "fold_metrics_filename = \"./Metrics/xgb_folds_summary_hist.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicitly ordered categorical columns\n",
    "ORDINAL_COLUMNS = {\n",
    "    'FAMI_ESTRATOVIVIENDA': [\n",
    "        'Sin Estrato',\n",
    "        'Estrato 1',\n",
    "        'Estrato 2',\n",
    "        'Estrato 3',\n",
    "        'Estrato 4',\n",
    "        'Estrato 5',\n",
    "        'Estrato 6'\n",
    "    ],\n",
    "\n",
    "    'FAMI_EDUCACIONPADRE': [\n",
    "        'Ninguno',\n",
    "        'Primaria incompleta',\n",
    "        'Primaria completa',\n",
    "        'Secundaria (Bachillerato) incompleta',\n",
    "        'Secundaria (Bachillerato) completa',\n",
    "        'Técnica o tecnológica incompleta',\n",
    "        'Técnica o tecnológica completa',\n",
    "        'Educación profesional incompleta',\n",
    "        'Educación profesional completa',\n",
    "        'Postgrado',\n",
    "        'No sabe',\n",
    "        'No Aplica'\n",
    "    ],\n",
    "\n",
    "    'FAMI_EDUCACIONMADRE': [\n",
    "        'Ninguno',\n",
    "        'Primaria incompleta',\n",
    "        'Primaria completa',\n",
    "        'Secundaria (Bachillerato) incompleta',\n",
    "        'Secundaria (Bachillerato) completa',\n",
    "        'Técnica o tecnológica incompleta',\n",
    "        'Técnica o tecnológica completa',\n",
    "        'Educación profesional incompleta',\n",
    "        'Educación profesional completa',\n",
    "        'Postgrado',\n",
    "        'No sabe',\n",
    "        'No Aplica'\n",
    "    ],\n",
    "\n",
    "    'ESTU_HORASSEMANATRABAJA': [\n",
    "        '0',\n",
    "        'Menos de 10 horas',\n",
    "        'Entre 11 y 20 horas',\n",
    "        'Entre 21 y 30 horas',\n",
    "        'Más de 30 horas'\n",
    "    ],\n",
    "\n",
    "    'ESTU_VALORMATRICULAUNIVERSIDAD': [\n",
    "        \"No pagó matrícula\",\n",
    "        \"Menos de 500 mil\",\n",
    "        \"Entre 500 mil y menos de 1 millón\",\n",
    "        \"Entre 1 millón y menos de 2.5 millones\",\n",
    "        \"Entre 2.5 millones y menos de 4 millones\",\n",
    "        \"Entre 4 millones y menos de 5.5 millones\",\n",
    "        \"Entre 5.5 millones y menos de 7 millones\",\n",
    "        \"Más de 7 millones\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class XGBWrapper:\n",
    "    def __init__(self, params, numeric_features, categorical_features):\n",
    "        self.params = params.copy()\n",
    "        self.num_boost_round = self.params.pop(\"num_boost_round\", self.params.pop(\"n_estimators\", 100))\n",
    "        self.early_stopping_rounds = self.params.pop(\"early_stopping_rounds\", None)\n",
    "        self.numeric_features = numeric_features\n",
    "        self.categorical_features = categorical_features\n",
    "\n",
    "        self.ordinal_features = [col for col in categorical_features if col in ORDINAL_COLUMNS]\n",
    "        self.ohe_features = [col for col in categorical_features if col not in ORDINAL_COLUMNS]\n",
    "\n",
    "        self.preprocessor = None  \n",
    "        self.model = None\n",
    "        self.evals_result = None\n",
    "        self.best_iteration = None\n",
    "        self.best_score = None\n",
    "\n",
    "    def _build_preprocessor(self):\n",
    "        transformers = []\n",
    "\n",
    "        # Numéricas\n",
    "        transformers.append(\n",
    "            ('num', SimpleImputer(strategy='median'), self.numeric_features)\n",
    "        )\n",
    "\n",
    "        # One-hot nominal\n",
    "        if self.ohe_features:\n",
    "            transformers.append(\n",
    "                ('ohe_cat', Pipeline([\n",
    "                    ('imputer', SimpleImputer(strategy='constant', fill_value='Sin Dato')),\n",
    "                    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "                ]), self.ohe_features)\n",
    "            )\n",
    "\n",
    "        for col in self.ordinal_features:\n",
    "            if col not in ORDINAL_COLUMNS:\n",
    "                continue\n",
    "            categories = [ORDINAL_COLUMNS[col]] if ORDINAL_COLUMNS[col] is not None else 'auto'\n",
    "            transformers.append(\n",
    "                (f'ord_{col}', Pipeline([\n",
    "                    ('imputer', SimpleImputer(strategy='constant', fill_value='Sin Dato')),\n",
    "                    ('ord', OrdinalEncoder(\n",
    "                        categories=categories if categories != 'auto' else 'auto',\n",
    "                        handle_unknown='use_encoded_value',\n",
    "                        unknown_value=-1\n",
    "                    ))\n",
    "                ]), [col])\n",
    "            )\n",
    "\n",
    "        return ColumnTransformer(\n",
    "            transformers=transformers,\n",
    "            verbose_feature_names_out=False,\n",
    "            remainder='drop',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, X, y, eval_set=None, sample_weight=None):\n",
    "        self.X_columns_ = X.columns.tolist() \n",
    "        self.preprocessor = self._build_preprocessor().fit(X)\n",
    "        X_proc = self.preprocessor.transform(X)\n",
    "\n",
    "        if sample_weight is not None:\n",
    "            dtrain = xgb.DMatrix(X_proc, label=y, weight=sample_weight)\n",
    "        else:\n",
    "            dtrain = xgb.DMatrix(X_proc, label=y)\n",
    "\n",
    "        evals = [(dtrain, \"train\")]\n",
    "        evals_result = {}\n",
    "\n",
    "        if eval_set is not None:\n",
    "            X_val, y_val = eval_set[0]\n",
    "            X_val_proc = self.preprocessor.transform(X_val)\n",
    "            dval = xgb.DMatrix(X_val_proc, label=y_val)\n",
    "            evals.append((dval, \"validation\"))\n",
    "\n",
    "        train_params = {\n",
    "            'params': self.params,\n",
    "            'dtrain': dtrain,\n",
    "            'num_boost_round': self.num_boost_round,\n",
    "            'evals': evals,\n",
    "            'evals_result': evals_result,\n",
    "            'verbose_eval': False\n",
    "        }\n",
    "\n",
    "        if self.early_stopping_rounds is not None and len(evals) > 1:\n",
    "            train_params['early_stopping_rounds'] = self.early_stopping_rounds\n",
    "\n",
    "        self.model = xgb.train(**train_params)\n",
    "        self.evals_result = evals_result\n",
    "        self.best_iteration = getattr(self.model, \"best_iteration\", None)\n",
    "        self.best_score = getattr(self.model, \"best_score\", None)                           \n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_proc = self.preprocessor.transform(X)\n",
    "        dmatrix = xgb.DMatrix(X_proc)\n",
    "        y_proba = self.model.predict(dmatrix)\n",
    "        return np.argmax(y_proba, axis=1)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X_proc = self.preprocessor.transform(X)\n",
    "        dmatrix = xgb.DMatrix(X_proc)\n",
    "        return self.model.predict(dmatrix)\n",
    "\n",
    "    def save(self, path):\n",
    "        joblib.dump({\n",
    "            \"params\": self.params,\n",
    "            \"num_boost_round\": self.num_boost_round,\n",
    "            \"early_stopping_rounds\": self.early_stopping_rounds,\n",
    "            \"numeric_features\": self.numeric_features,\n",
    "            \"categorical_features\": self.categorical_features,\n",
    "            \"preprocessor\": self.preprocessor,\n",
    "            \"booster\": self.model,\n",
    "            \"best_iteration\": self.best_iteration,\n",
    "            \"best_score\": self.best_score\n",
    "        }, path)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        data = joblib.load(path)\n",
    "        params = {**data[\"params\"], \"num_boost_round\": data[\"num_boost_round\"]}\n",
    "\n",
    "        if \"early_stopping_rounds\" in data:\n",
    "            params[\"early_stopping_rounds\"] = data[\"early_stopping_rounds\"]\n",
    "\n",
    "        wrapper = cls(\n",
    "            params,\n",
    "            data[\"numeric_features\"],\n",
    "            data[\"categorical_features\"]\n",
    "        )\n",
    "        wrapper.preprocessor = data[\"preprocessor\"]\n",
    "        wrapper.model = data[\"booster\"]\n",
    "        wrapper.best_iteration = data.get(\"best_iteration\")\n",
    "        wrapper.best_score = data.get(\"best_score\")\n",
    "        return wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# MODEL EVALUATION\n",
    "# ============================\n",
    "def evaluate_model(model, X_data, y_data):\n",
    "    y_pred = model.predict(X_data)\n",
    "    y_proba = model.predict_proba(X_data)\n",
    "    f1 = f1_score(y_data, y_pred, average='weighted')\n",
    "    acc = accuracy_score(y_data, y_pred)\n",
    "    loss = log_loss(y_data, y_proba)\n",
    "    auc = roc_auc_score(y_data, y_proba, multi_class='ovr', average='weighted')\n",
    "    report = classification_report(y_data, y_pred)\n",
    "    cm = confusion_matrix(y_data, y_pred)\n",
    "    return f1, acc, loss, auc, report, cm, y_proba, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# LIME EXPLAINER BUILDER\n",
    "# ============================\n",
    "def get_lime_explainer(model_wrapper, X_train_raw, y_train_raw):\n",
    "    X_transformed = model_wrapper.preprocessor.transform(X_train_raw)\n",
    "    feature_names = model_wrapper.preprocessor.get_feature_names_out()\n",
    "    class_names = np.unique(y_train_raw).astype(str)\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "        training_data=X_transformed,\n",
    "        feature_names=feature_names,\n",
    "        class_names=class_names,\n",
    "        mode='classification'\n",
    "    )\n",
    "    return explainer, X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# OPTUNA INNER CV OPTIMIZATION\n",
    "# ============================\n",
    "def run_inner_optuna(X_inner, y_inner, numeric_features, categorical_features, n_trials=50):\n",
    "    def objective(trial):\n",
    "        \n",
    "        num_boost_round = trial.suggest_int(\"num_boost_round\", 50, 250)\n",
    "\n",
    "        params = {\n",
    "            \"verbosity\": 0,\n",
    "            \"objective\": \"multi:softprob\",\n",
    "            \"num_class\": len(np.unique(y_inner)),\n",
    "            \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"dart\"]),\n",
    "            \"lambda\": trial.suggest_float(\"lambda\", 1e-3, 10.0, log=True),\n",
    "            \"alpha\": trial.suggest_float(\"alpha\", 1e-3, 10.0, log=True),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 4, 8),\n",
    "            \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "            \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1),\n",
    "            \"tree_method\": \"hist\", #\"tree_method\": \"hist\", # Colocar este si falla\n",
    "            \"device\": \"cuda\",\n",
    "            \"seed\": SEED\n",
    "        }\n",
    "\n",
    "        # Extra dropout config if booster is dart\n",
    "        if params[\"booster\"] == \"dart\":\n",
    "            params[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "            params[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "            params[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 0.1, 0.5)\n",
    "            params[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 0.0, 0.5)\n",
    "        \n",
    "        params[\"num_boost_round\"] = num_boost_round\n",
    "        \n",
    "        model = XGBWrapper(params, numeric_features, categorical_features)\n",
    "        skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "        scores = []\n",
    "        \n",
    "        for train_idx, val_idx in skf.split(X_inner, y_inner):\n",
    "            X_t, X_v = X_inner.iloc[train_idx], X_inner.iloc[val_idx]\n",
    "            y_t, y_v = y_inner.iloc[train_idx], y_inner.iloc[val_idx]\n",
    "            \n",
    "            sample_weights = compute_sample_weight(\"balanced\", y_t)\n",
    "            \n",
    "            eval_set = [(X_v, y_v)]\n",
    "            \n",
    "            model.fit(X_t, y_t, eval_set=eval_set, sample_weight=sample_weights)\n",
    "            \n",
    "            y_pred = model.predict(X_v)\n",
    "            scores.append(f1_score(y_v, y_pred, average='weighted'))\n",
    "        \n",
    "        return np.mean(scores)\n",
    "    \n",
    "    pruner = MedianPruner()\n",
    "    study = optuna.create_study(direction=\"maximize\", pruner=pruner)\n",
    "    study.optimize(objective, n_trials=n_trials, n_jobs=10)\n",
    "    \n",
    "    return study.best_params, study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# FOLD METRIC SAVER\n",
    "# ============================\n",
    "def save_metrics_folds(folds_metrics: list, filename: str) -> pd.DataFrame:\n",
    "    df = pd.DataFrame(folds_metrics)\n",
    "    metric_cols = df.columns.drop('fold') if 'fold' in df.columns else df.columns\n",
    "    mean_row = df[metric_cols].mean().to_dict()\n",
    "    std_row = df[metric_cols].std().to_dict()\n",
    "    mean_row['fold'] = 'mean'\n",
    "    std_row['fold'] = 'std'\n",
    "    df_final = pd.concat([df, pd.DataFrame([mean_row, std_row])], ignore_index=True)\n",
    "    df_final.to_csv(filename, index=False)\n",
    "    print(f\"\\n📁 Fold metrics + summary saved to: {filename}\")\n",
    "    return df_final\n",
    "\n",
    "# ============================\n",
    "# CONFUSION MATRIX PLOTTER\n",
    "# ============================\n",
    "def plot_confusion_matrix(cm, labels, title):\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================\n",
    "# NESTED CV\n",
    "# ============================\n",
    "def nested_cv(X: pd.DataFrame, y: pd.Series, numeric_features: list, categorical_features: list):\n",
    "    visualizations = {}\n",
    "    all_folds_metrics = []\n",
    "\n",
    "    if os.path.exists(model_filename) and os.path.exists(metrics_filename):\n",
    "        best_model = joblib.load(model_filename)\n",
    "        best_metrics = joblib.load(metrics_filename)\n",
    "        if \"best_fold\" in best_metrics:\n",
    "            study_dt = joblib.load(study_filename)['studies'][best_metrics['best_fold']]\n",
    "        else:\n",
    "            study_dt = None\n",
    "        if study_dt:\n",
    "            visualizations['optimization_history'] = vis.plot_optimization_history(study_dt)\n",
    "            visualizations['parallel_coordinate'] = vis.plot_parallel_coordinate(study_dt)\n",
    "            visualizations['param_importances'] = vis.plot_param_importances(study_dt)\n",
    "            f1_scores = [t.value for t in study_dt.trials]\n",
    "            fig = go.Figure()\n",
    "            fig.add_trace(go.Scatter(x=list(range(len(f1_scores))), y=f1_scores, mode='lines+markers', name='F1-score'))\n",
    "            fig.update_layout(title='F1-Score Evolution During Optuna Optimization', xaxis_title='Trial', yaxis_title='F1-Score', template='plotly_dark')\n",
    "            visualizations['f1_score_evolution'] = fig\n",
    "        return best_model, best_metrics, study_dt, visualizations\n",
    "\n",
    "    optuna_studies = []\n",
    "    outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    best_f1 = -np.inf\n",
    "    best_model = None\n",
    "    best_metrics = {}\n",
    "\n",
    "    for fold, (train_idx, test_idx) in tqdm(\n",
    "        enumerate(outer_cv.split(X, y), 1),\n",
    "        total = outer_cv.get_n_splits(),\n",
    "        desc=\"Training folds\"\n",
    "    ):\n",
    "        \n",
    "        X_train_fold, X_test_fold = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train_fold, y_test_fold = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "        X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "            X_train_fold, y_train_fold,\n",
    "            test_size=0.2, stratify=y_train_fold, random_state=SEED\n",
    "        )\n",
    "\n",
    "        start_time = time.time()\n",
    "        best_params, study = run_inner_optuna(X_tr, y_tr, numeric_features, categorical_features)\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        optuna_studies.append(study)\n",
    "        \n",
    "        best_params[\"objective\"] = \"multi:softprob\"\n",
    "        best_params[\"num_class\"] = 5       \n",
    "\n",
    "        model = XGBWrapper(best_params, numeric_features, categorical_features)\n",
    "        model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)])\n",
    "\n",
    "        f1_train, acc_train, loss_train, auc_train, report_train, cm_train, y_proba_train, y_train_pred = evaluate_model(model, X_tr, y_tr)\n",
    "        f1_val, acc_val, loss_val, auc_val, report_val, cm_val, y_proba_val, y_val_pred = evaluate_model(model, X_val, y_val)\n",
    "        f1_test, acc_test, loss_test, auc_test, report_test, cm_test, y_proba_test, y_test_pred = evaluate_model(model, X_test_fold, y_test_fold)\n",
    "\n",
    "        fold_metrics = {\n",
    "            \"fold\": fold,\n",
    "            \"f1_train\": f1_train,\n",
    "            \"accuracy_train\": acc_train,\n",
    "            \"log_loss_train\": loss_train,\n",
    "            \"auc_train\": auc_train,\n",
    "            \"f1_val\": f1_val,\n",
    "            \"accuracy_val\": acc_val,\n",
    "            \"log_loss_val\": loss_val,\n",
    "            \"auc_val\": auc_val,\n",
    "            \"f1_test\": f1_test,\n",
    "            \"accuracy_test\": acc_test,\n",
    "            \"log_loss_test\": loss_test,\n",
    "            \"auc_test\": auc_test,\n",
    "            \"optuna_time\": elapsed\n",
    "        }\n",
    "        all_folds_metrics.append(fold_metrics)\n",
    "\n",
    "        if f1_test > best_f1:\n",
    "            best_f1 = f1_test\n",
    "            best_model = model\n",
    "            best_metrics = {\n",
    "                **fold_metrics,\n",
    "                \"params\": best_params,\n",
    "                \"labels\": np.unique(y),\n",
    "                \"best_fold\": fold - 1,\n",
    "                \"X_train_fold\": X_tr,\n",
    "                \"y_train_fold\": y_tr,\n",
    "                \"y_train_pred\": y_train_pred,\n",
    "                \"y_train_true\": y_tr,\n",
    "                \"y_proba_train\": y_proba_train,\n",
    "                \"classification_report_train\": report_train,\n",
    "                \"confusion_matrix_train\": cm_train,\n",
    "                \"y_val_true\": y_val,\n",
    "                \"y_val_pred\": y_val_pred,\n",
    "                \"y_proba_val\": y_proba_val,\n",
    "                \"classification_report_val\": report_val,\n",
    "                \"confusion_matrix_val\": cm_val,\n",
    "                \"y_test_fold\": y_test_fold,\n",
    "                \"y_test_pred\": y_test_pred,\n",
    "                \"y_proba_test\": y_proba_test,\n",
    "                \"classification_report_test\": report_test,\n",
    "                \"confusion_matrix_test\": cm_test\n",
    "            }\n",
    "            \n",
    "    joblib.dump(best_model, model_filename)\n",
    "    joblib.dump({\"studies\": optuna_studies}, study_filename)\n",
    "    joblib.dump(best_metrics, metrics_filename)\n",
    "    df_folds = save_metrics_folds(all_folds_metrics, fold_metrics_filename)\n",
    "\n",
    "    study_dt = optuna_studies[best_metrics['best_fold']]\n",
    "    visualizations['optimization_history'] = vis.plot_optimization_history(study_dt)\n",
    "    visualizations['parallel_coordinate'] = vis.plot_parallel_coordinate(study_dt)\n",
    "    visualizations['param_importances'] = vis.plot_param_importances(study_dt)\n",
    "\n",
    "    f1_scores = [t.value for t in study_dt.trials]\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=list(range(len(f1_scores))), y=f1_scores, mode='lines+markers', name='F1-score'))\n",
    "    fig.update_layout(title='F1-Score Evolution During Optuna Optimization', xaxis_title='Trial', yaxis_title='F1-Score', template='plotly_dark')\n",
    "    visualizations['f1_score_evolution'] = fig\n",
    "\n",
    "    return best_model, best_metrics, study_dt, visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pipeline\n",
    "model, metrics, study, visualizations = nested_cv(X, y, numeric_features, categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics['classification_report_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(metrics['confusion_matrix_train'], metrics['labels'], \"Train Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics['classification_report_val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(metrics['confusion_matrix_val'], metrics['labels'], \"Validation Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics['classification_report_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(metrics['confusion_matrix_test'], metrics['labels'], \"Test Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curves(model):\n",
    "    \"\"\"\n",
    "    Plots train and validation log loss over boosting rounds.\n",
    "    Adds a vertical line at the best_iteration if available.\n",
    "    \"\"\"\n",
    "    if not hasattr(model, \"evals_result\"):\n",
    "        raise ValueError(\"The model has no evaluation history.\")\n",
    "\n",
    "    results = model.evals_result\n",
    "    for dataset in results:\n",
    "        if \"mlogloss\" in results[dataset]:\n",
    "            plt.plot(results[dataset][\"mlogloss\"], label=f\"{dataset} logloss\")\n",
    "\n",
    "    plt.xlabel(\"Boosting round\")\n",
    "    plt.ylabel(\"Log loss\")\n",
    "    plt.title(\"Log loss - train vs validation\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    if hasattr(model, \"best_iteration\") and model.best_iteration is not None:\n",
    "        plt.axvline(x=model.best_iteration, color=\"red\", linestyle=\"--\", label=\"Best iteration\")\n",
    "        plt.legend()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed = model.preprocessor.transform(metrics['X_train_fold'])\n",
    "\n",
    "def predict_fn(x):\n",
    "    dmatrix = xgb.DMatrix(x)\n",
    "    return model.model.predict(dmatrix)\n",
    "\n",
    "feature_names = model.preprocessor.get_feature_names_out()\n",
    "class_names = np.unique(metrics['y_train_fold']).astype(str)\n",
    "\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "    training_data=X_transformed,\n",
    "    feature_names=feature_names,\n",
    "    class_names=class_names,\n",
    "    mode='classification'\n",
    ")\n",
    "\n",
    "instance = X_transformed[10]\n",
    "exp = explainer.explain_instance(instance, predict_fn=predict_fn, labels=[0, 1, 2])\n",
    "\n",
    "exp.show_in_notebook(show_table=True)\n",
    "\n",
    "for class_label in exp.available_labels():\n",
    "    print(f\"--- Explanation for class {class_label} ---\")\n",
    "    fig = exp.as_pyplot_figure(label=class_label)\n",
    "    plt.title(f\"LIME - Class {class_label}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizations.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizations['optimization_history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizations['parallel_coordinate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizations['f1_score_evolution']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizations['param_importances']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_importances = model.model.get_score(importance_type='gain')\n",
    "\n",
    "feature_names = model.preprocessor.get_feature_names_out()\n",
    "\n",
    "mapped_importances = []\n",
    "for i, fname in enumerate(feature_names):\n",
    "    score = raw_importances.get(f\"f{i}\", 0)\n",
    "    mapped_importances.append((fname, score))\n",
    "\n",
    "importance_df = pd.DataFrame(mapped_importances, columns=[\"Feature\", \"Importance\"])\n",
    "importance_df = importance_df.sort_values(by=\"Importance\", ascending=False).head(20)\n",
    "\n",
    "fig = px.bar(\n",
    "    importance_df,\n",
    "    x='Feature',\n",
    "    y='Importance',\n",
    "    title='Top 20 Feature Importance - XGBoost',\n",
    "    labels={'Feature': 'Feature Name', 'Importance': 'Importance Score'},\n",
    "    color='Importance',\n",
    "    color_continuous_scale='Viridis',\n",
    "    text='Importance'\n",
    ")\n",
    "fig.update_layout(xaxis_tickangle=45)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reporte.save(metrics, model_name=\"XGBOOST_HIST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = reporte.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"Model == 'XGBOOST_HIST' and Type == 'train' and Class != 'global'\").iloc[:, 0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"Model == 'XGBOOST_HIST' and Type == 'val' and Class != 'global'\").iloc[:, 0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"Model == 'XGBOOST_HIST' and Type == 'test' and Class != 'global'\").iloc[:, 0:6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"Model == 'XGBOOST_HIST' and auc != '-'\")[[\"Model\", \"Type\", \"accuracy\", \"log_loss\", \"auc\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"Model == 'XGBOOST_HIST' and Type == 'val' and Class\t== 'global'\")[[\"Model\", \"Type\", \"accuracy\", \"log_loss\", \"auc\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_multiclass(y_true, y_proba, class_labels, title=\"AUC-ROC Curve (Multiclass)\"):\n",
    "    # Binarize true labels\n",
    "    y_bin = label_binarize(y_true, classes=class_labels)\n",
    "    n_classes = len(class_labels)\n",
    "\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], y_proba[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for i in range(n_classes):\n",
    "        plt.plot(fpr[i], tpr[i],\n",
    "                 label=f\"Class {class_labels[i]} (AUC = {roc_auc[i]:.3f})\")\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", label=\"Random\")\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_multiclass(\n",
    "    y_true=metrics[\"y_test_fold\"],\n",
    "    y_proba=metrics[\"y_proba_test\"],\n",
    "    class_labels=metrics[\"labels\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.params  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
