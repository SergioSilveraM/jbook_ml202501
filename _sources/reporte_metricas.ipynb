{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Custom Class Metrics***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook se encuentra la clase personalizada empleada para el cálculo, guardado y carga de las métricas para cada uno de los modelos empleados en este proyecto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "class ReporteMetricas:\n",
    "    def __init__(self, output_dir: str = \"../Metrics\"):\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "    def save(self, metrics: dict, model_name: str):\n",
    "        rows = []\n",
    "        metric_cols = [\"precision\", \"recall\", \"f1-score\", \"support\"]\n",
    "\n",
    "        for tipo in [\"train\", \"val\", \"test\"]:\n",
    "            if tipo == \"train\" and \"accuracy_train\" in metrics:\n",
    "                y_true = metrics.get(\"y_train_true\", metrics[\"y_train_fold\"])\n",
    "                y_pred = metrics.get(\"y_train_pred\")\n",
    "                acc = metrics[\"accuracy_train\"]\n",
    "                log = metrics[\"log_loss_train\"]\n",
    "                auc = metrics[\"auc_train\"]\n",
    "            elif tipo == \"val\" and \"accuracy_val\" in metrics:\n",
    "                y_true = metrics.get(\"y_val_true\", metrics.get(\"y_train_fold\"))\n",
    "                y_pred = metrics.get(\"y_val_pred\")\n",
    "                acc = metrics[\"accuracy_val\"]\n",
    "                log = metrics[\"log_loss_val\"]\n",
    "                auc = metrics[\"auc_val\"]\n",
    "            elif tipo == \"test\" and \"accuracy_test\" in metrics:\n",
    "                y_true = metrics.get(\"y_test_fold\")\n",
    "                y_pred = metrics.get(\"y_test_pred\")\n",
    "                acc = metrics[\"accuracy_test\"]\n",
    "                log = metrics[\"log_loss_test\"]\n",
    "                auc = metrics[\"auc_test\"]\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            if y_true is None or y_pred is None:\n",
    "                continue\n",
    "\n",
    "            report_dict = classification_report(y_true, y_pred, output_dict=True)\n",
    "            report_dict.pop(\"accuracy\", None)\n",
    "\n",
    "            df = pd.DataFrame(report_dict).T.reset_index()\n",
    "            df.rename(columns={\"index\": \"Class\"}, inplace=True)\n",
    "            df[\"Model\"] = model_name\n",
    "            df[\"Type\"] = tipo\n",
    "            df[\"accuracy\"] = \"-\"\n",
    "            df[\"log_loss\"] = \"-\"\n",
    "            df[\"auc\"] = \"-\"\n",
    "\n",
    "            df = df[[\"Model\", \"Type\", \"Class\"] + metric_cols + [\"accuracy\", \"log_loss\", \"auc\"]]\n",
    "\n",
    "            for col in metric_cols:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce').round(3)\n",
    "            df[metric_cols] = df[metric_cols].fillna(0)\n",
    "\n",
    "            global_row = pd.DataFrame([{\n",
    "                \"Model\": model_name,\n",
    "                \"Type\": tipo,\n",
    "                \"Class\": \"global\",\n",
    "                \"precision\": \"-\",\n",
    "                \"recall\": \"-\",\n",
    "                \"f1-score\": \"-\",\n",
    "                \"support\": \"-\",\n",
    "                \"accuracy\": round(acc, 3),\n",
    "                \"log_loss\": round(log, 3),\n",
    "                \"auc\": round(auc, 3)\n",
    "            }])\n",
    "\n",
    "            df = pd.concat([df, global_row], ignore_index=True)\n",
    "            rows.append(df)\n",
    "\n",
    "        if not rows:\n",
    "            print(f\"No metrics saved for {model_name}. Check the dict keys.\")\n",
    "            return\n",
    "\n",
    "        df_final = pd.concat(rows, ignore_index=True)\n",
    "\n",
    "        # Save CSV\n",
    "        csv_path = os.path.join(self.output_dir, f\"Metrics_{model_name}.csv\")\n",
    "        df_final.to_csv(csv_path, index=False)\n",
    "\n",
    "        # Save JSON\n",
    "        json_path = os.path.join(self.output_dir, f\"Metrics_{model_name}.json\")\n",
    "        df_final.to_json(json_path, orient=\"records\", indent=4)\n",
    "\n",
    "        print(f\"\\nReport for model '{model_name}' saved:\")\n",
    "        print(f\"   → CSV: {csv_path}\")\n",
    "        print(f\"   → JSON: {json_path}\")\n",
    "\n",
    "    def load(self):\n",
    "        files = [f for f in os.listdir(self.output_dir) if f.endswith(\".csv\") and f.startswith(\"Metrics_\")]\n",
    "        if not files:\n",
    "            print(\"No metric reports found.\")\n",
    "            return None\n",
    "\n",
    "        all_reports = pd.concat([pd.read_csv(os.path.join(self.output_dir, f)) for f in files], ignore_index=True)\n",
    "        print(f\"\\nLoaded {len(files)} report(s):\")\n",
    "        print(f\"   → Models: {all_reports['Model'].unique().tolist()}\")\n",
    "        print(f\"   → Types : {all_reports['Type'].unique().tolist()}\")\n",
    "        return all_reports\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
