{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***RNN***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# IMPORTS\n",
    "# ============================\n",
    "import os\n",
    "import time\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import random\n",
    "import plotly.express as px\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, label_binarize, OrdinalEncoder, MinMaxScaler, LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.metrics import (\n",
    "    f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    accuracy_score, log_loss, roc_auc_score, roc_curve, auc\n",
    ")\n",
    "\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "import optuna.visualization as vis\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "\n",
    "\n",
    "from reporte_metricas import ReporteMetricas\n",
    "reporte = ReporteMetricas()\n",
    "\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, log_loss, roc_auc_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Input, Dense, Dropout, LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =======================\n",
    "# LOAD DATA\n",
    "# =======================\n",
    "file_path = \"../Saber_pro_sampled_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "#df = df.head(1000)\n",
    "X = df.drop(columns=[\"MOD_INGLES_DESEM\"])\n",
    "y = df[\"MOD_INGLES_DESEM\"]\n",
    "\n",
    "categorical_features = X.select_dtypes(include='object').columns.tolist()\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = pd.Series(le.fit_transform(y), index=y.index)\n",
    "\n",
    "class_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(\"Class mapping:\", class_mapping)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# FILE PATHS\n",
    "# ============================\n",
    "model_filename = \"../Models/best_rnn_model.pkl\"\n",
    "study_filename = \"../Study/optuna_study_RNN_nested.pkl\"\n",
    "metrics_filename = \"../Metrics/best_rnn_metrics.pkl\"\n",
    "fold_metrics_filename = \"../Metrics/rnn_folds_summary.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicitly ordered categorical columns\n",
    "ORDINAL_COLUMNS = {\n",
    "    'FAMI_ESTRATOVIVIENDA': [\n",
    "        'Sin Estrato',\n",
    "        'Estrato 1',\n",
    "        'Estrato 2',\n",
    "        'Estrato 3',\n",
    "        'Estrato 4',\n",
    "        'Estrato 5',\n",
    "        'Estrato 6'\n",
    "    ],\n",
    "\n",
    "    'FAMI_EDUCACIONPADRE': [\n",
    "        'Ninguno',\n",
    "        'Primaria incompleta',\n",
    "        'Primaria completa',\n",
    "        'Secundaria (Bachillerato) incompleta',\n",
    "        'Secundaria (Bachillerato) completa',\n",
    "        'Técnica o tecnológica incompleta',\n",
    "        'Técnica o tecnológica completa',\n",
    "        'Educación profesional incompleta',\n",
    "        'Educación profesional completa',\n",
    "        'Postgrado',\n",
    "        'No sabe',\n",
    "        'No Aplica'\n",
    "    ],\n",
    "\n",
    "    'FAMI_EDUCACIONMADRE': [\n",
    "        'Ninguno',\n",
    "        'Primaria incompleta',\n",
    "        'Primaria completa',\n",
    "        'Secundaria (Bachillerato) incompleta',\n",
    "        'Secundaria (Bachillerato) completa',\n",
    "        'Técnica o tecnológica incompleta',\n",
    "        'Técnica o tecnológica completa',\n",
    "        'Educación profesional incompleta',\n",
    "        'Educación profesional completa',\n",
    "        'Postgrado',\n",
    "        'No sabe',\n",
    "        'No Aplica'\n",
    "    ],\n",
    "\n",
    "    'ESTU_HORASSEMANATRABAJA': [\n",
    "        '0',\n",
    "        'Menos de 10 horas',\n",
    "        'Entre 11 y 20 horas',\n",
    "        'Entre 21 y 30 horas',\n",
    "        'Más de 30 horas'\n",
    "    ],\n",
    "\n",
    "    'ESTU_VALORMATRICULAUNIVERSIDAD': [\n",
    "        \"No pagó matrícula\",\n",
    "        \"Menos de 500 mil\",\n",
    "        \"Entre 500 mil y menos de 1 millón\",\n",
    "        \"Entre 1 millón y menos de 2.5 millones\",\n",
    "        \"Entre 2.5 millones y menos de 4 millones\",\n",
    "        \"Entre 4 millones y menos de 5.5 millones\",\n",
    "        \"Entre 5.5 millones y menos de 7 millones\",\n",
    "        \"Más de 7 millones\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNWrapper:\n",
    "    def __init__(self, params, numeric_features, categorical_features):\n",
    "        self.params = params.copy()\n",
    "        self.numeric_features = numeric_features\n",
    "        self.categorical_features = categorical_features\n",
    "        self.preprocessor = self._build_preprocessor()\n",
    "        self.model = None\n",
    "\n",
    "    def _build_preprocessor(self):\n",
    "        ordinal_features = [col for col in self.categorical_features if col in ORDINAL_COLUMNS]\n",
    "        nominal_features = [col for col in self.categorical_features if col not in ORDINAL_COLUMNS]\n",
    "\n",
    "        transformers = []\n",
    "\n",
    "        for col in ordinal_features:\n",
    "            order = ORDINAL_COLUMNS[col]\n",
    "            transformers.append(\n",
    "                (f'ord_{col}', Pipeline([\n",
    "                    ('imputer', SimpleImputer(strategy='constant', fill_value='Sin Dato')),\n",
    "                    ('ord', OrdinalEncoder(categories=[order], handle_unknown='use_encoded_value', unknown_value=-1)),\n",
    "                    ('scaler', MinMaxScaler()) \n",
    "                ]), [col])\n",
    "            )\n",
    "\n",
    "        if nominal_features:\n",
    "            transformers.append(\n",
    "                ('nominal', Pipeline([\n",
    "                    ('imputer', SimpleImputer(strategy='constant', fill_value='Sin Dato')),\n",
    "                    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "                ]), nominal_features)\n",
    "            )\n",
    "\n",
    "        return ColumnTransformer(\n",
    "            transformers=transformers,\n",
    "            verbose_feature_names_out=False,\n",
    "            remainder='drop',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "    def _reshape_for_rnn(self, X):\n",
    "        return X.reshape((X.shape[0], 1, X.shape[1]))\n",
    "\n",
    "    def fit(self, X, y, eval_set=None, sample_weight=None):\n",
    "        self.preprocessor = self._build_preprocessor().fit(X)\n",
    "        X_proc = self.preprocessor.transform(X)\n",
    "        X_seq = self._reshape_for_rnn(X_proc)\n",
    "        y_cat = to_categorical(y)\n",
    "\n",
    "        input_shape = (X_seq.shape[1], X_seq.shape[2])\n",
    "        num_classes = y_cat.shape[1]\n",
    "\n",
    "        device_name = self.params.get(\"device\", \"/GPU:0\" if tf.config.list_physical_devices(\"GPU\") else \"/CPU:0\")\n",
    "\n",
    "        with tf.device(device_name):\n",
    "            self.model = Sequential()\n",
    "            self.model.add(Input(shape=input_shape))\n",
    "            self.model.add(SimpleRNN(\n",
    "                units=self.params['units'],\n",
    "                dropout=self.params['dropout'],\n",
    "                recurrent_dropout=self.params['recurrent_dropout'],\n",
    "                return_sequences=False\n",
    "            ))\n",
    "            self.model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "            self.model.compile(\n",
    "                optimizer=self.params['optimizer'],\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=[\n",
    "                    'accuracy',\n",
    "                    AUC(name='auc', multi_label=True, num_labels=num_classes)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            callbacks = []\n",
    "            if self.params.get(\"early_stopping\", True):\n",
    "                callbacks.append(EarlyStopping(\n",
    "                    monitor='val_auc',\n",
    "                    patience=self.params.get(\"patience\", 5),\n",
    "                    restore_best_weights=True\n",
    "                ))\n",
    "\n",
    "            if eval_set:\n",
    "                X_val, y_val = eval_set[0]\n",
    "                X_val_proc = self.preprocessor.transform(X_val)\n",
    "                X_val_seq = self._reshape_for_rnn(X_val_proc)\n",
    "                y_val_cat = to_categorical(y_val)\n",
    "            else:\n",
    "                X_val_seq, y_val_cat = None, None\n",
    "\n",
    "            self.history = self.model.fit(\n",
    "                X_seq,\n",
    "                y_cat,\n",
    "                validation_data=(X_val_seq, y_val_cat) if eval_set else None,\n",
    "                sample_weight=sample_weight,\n",
    "                epochs=self.params['epochs'],\n",
    "                batch_size=self.params['batch_size'],\n",
    "                verbose=0,\n",
    "                callbacks=callbacks\n",
    "            )\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_proc = self.preprocessor.transform(X)\n",
    "        X_seq = self._reshape_for_rnn(X_proc)\n",
    "        return np.argmax(self.model.predict(X_seq), axis=1)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X_proc = self.preprocessor.transform(X)\n",
    "        X_seq = self._reshape_for_rnn(X_proc)\n",
    "        return self.model.predict(X_seq)\n",
    "\n",
    "    def save(self, path):\n",
    "        self.model.save(f\"{path}_model.h5\")\n",
    "        joblib.dump({\n",
    "            \"params\": self.params,\n",
    "            \"numeric_features\": self.numeric_features,\n",
    "            \"categorical_features\": self.categorical_features,\n",
    "            \"preprocessor\": self.preprocessor,\n",
    "        }, f\"{path}_meta.pkl\")\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        data = joblib.load(f\"{path}_meta.pkl\")\n",
    "        wrapper = cls(data[\"params\"], data[\"numeric_features\"], data[\"categorical_features\"])\n",
    "        wrapper.preprocessor = data[\"preprocessor\"]\n",
    "        wrapper.model = load_model(f\"{path}_model.h5\")\n",
    "        return wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# MODEL EVALUATION\n",
    "# ============================\n",
    "def evaluate_model(model, X_data, y_data):\n",
    "    y_pred = model.predict(X_data)\n",
    "    y_proba = model.predict_proba(X_data)\n",
    "    f1 = f1_score(y_data, y_pred, average='weighted')\n",
    "    acc = accuracy_score(y_data, y_pred)\n",
    "    loss = log_loss(y_data, y_proba)\n",
    "    auc = roc_auc_score(y_data, y_proba, multi_class='ovr', average='weighted')\n",
    "    report = classification_report(y_data, y_pred)\n",
    "    cm = confusion_matrix(y_data, y_pred)\n",
    "    return f1, acc, loss, auc, report, cm, y_proba, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# LIME EXPLAINER BUILDER\n",
    "# ============================\n",
    "def get_lime_explainer(model_wrapper, X_train_raw, y_train_raw):\n",
    "    \"\"\"\n",
    "    LIME explainer builder for MLP model wrapper (TensorFlow).\n",
    "    \"\"\"\n",
    "    X_transformed = model_wrapper.preprocessor.transform(X_train_raw)\n",
    "    feature_names = model_wrapper.preprocessor.get_feature_names_out()\n",
    "    class_names = np.unique(y_train_raw).astype(str)\n",
    "\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "        training_data=X_transformed,\n",
    "        feature_names=feature_names,\n",
    "        class_names=class_names,\n",
    "        mode='classification'\n",
    "    )\n",
    "    return explainer, X_transformed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inner_optuna(X_inner, y_inner, numeric_features, categorical_features, n_trials=50):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"units\": trial.suggest_int(\"units\", 64, 256, step=32),\n",
    "            \"dropout\": trial.suggest_float(\"dropout\", 0.1, 0.5),\n",
    "            \"recurrent_dropout\": trial.suggest_float(\"recurrent_dropout\", 0.0, 0.5),\n",
    "            \"optimizer\": trial.suggest_categorical(\"optimizer\", [\"adam\", \"rmsprop\", \"sgd\"]),\n",
    "            \"epochs\": trial.suggest_int(\"epochs\", 10, 70),\n",
    "            \"batch_size\": trial.suggest_int(\"batch_size\", 32, 256, log=True),\n",
    "            \"early_stopping\": True,\n",
    "            \"patience\": trial.suggest_int(\"patience\", 5, 15)\n",
    "        }\n",
    "\n",
    "        model = RNNWrapper(params, numeric_features, categorical_features)\n",
    "        skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "        scores = []\n",
    "\n",
    "        for train_idx, val_idx in skf.split(X_inner, y_inner):\n",
    "            X_t, X_v = X_inner.iloc[train_idx], X_inner.iloc[val_idx]\n",
    "            y_t, y_v = y_inner.iloc[train_idx], y_inner.iloc[val_idx]\n",
    "            sample_weights = compute_sample_weight(\"balanced\", y_t)\n",
    "\n",
    "            model.fit(X_t, y_t, eval_set=[(X_v, y_v)], sample_weight=sample_weights)\n",
    "            y_pred = model.predict(X_v)\n",
    "            scores.append(f1_score(y_v, y_pred, average='weighted'))\n",
    "\n",
    "        return np.mean(scores)\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\", pruner=MedianPruner())\n",
    "    study.optimize(objective, n_trials=n_trials, n_jobs=1)\n",
    "    return study.best_params, study\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# FOLD METRIC SAVER\n",
    "# ============================\n",
    "def save_metrics_folds(folds_metrics: list, filename: str) -> pd.DataFrame:\n",
    "    df = pd.DataFrame(folds_metrics)\n",
    "    metric_cols = df.columns.drop('fold') if 'fold' in df.columns else df.columns\n",
    "    mean_row = df[metric_cols].mean().to_dict()\n",
    "    std_row = df[metric_cols].std().to_dict()\n",
    "    mean_row['fold'] = 'mean'\n",
    "    std_row['fold'] = 'std'\n",
    "    df_final = pd.concat([df, pd.DataFrame([mean_row, std_row])], ignore_index=True)\n",
    "    df_final.to_csv(filename, index=False)\n",
    "    print(f\"\\n📁 Fold metrics + summary saved to: {filename}\")\n",
    "    return df_final\n",
    "\n",
    "# ============================\n",
    "# CONFUSION MATRIX PLOTTER\n",
    "# ============================\n",
    "def plot_confusion_matrix(cm, labels, title):\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================\n",
    "# NESTED CV\n",
    "# ============================\n",
    "def nested_cv(X: pd.DataFrame, y: pd.Series, numeric_features: list, categorical_features: list):\n",
    "    visualizations = {}\n",
    "    all_folds_metrics = []\n",
    "\n",
    "    if os.path.exists(model_filename) and os.path.exists(metrics_filename):\n",
    "        best_model = joblib.load(model_filename)\n",
    "        best_metrics = joblib.load(metrics_filename)\n",
    "        if \"best_fold\" in best_metrics:\n",
    "            study_dt = joblib.load(study_filename)['studies'][best_metrics['best_fold']]\n",
    "        else:\n",
    "            study_dt = None\n",
    "        if study_dt:\n",
    "            visualizations['optimization_history'] = vis.plot_optimization_history(study_dt)\n",
    "            visualizations['parallel_coordinate'] = vis.plot_parallel_coordinate(study_dt)\n",
    "            visualizations['param_importances'] = vis.plot_param_importances(study_dt)\n",
    "            f1_scores = [t.value for t in study_dt.trials]\n",
    "            fig = go.Figure()\n",
    "            fig.add_trace(go.Scatter(x=list(range(len(f1_scores))), y=f1_scores, mode='lines+markers', name='F1-score'))\n",
    "            fig.update_layout(title='F1-Score Evolution During Optuna Optimization', xaxis_title='Trial', yaxis_title='F1-Score', template='plotly_dark')\n",
    "            visualizations['f1_score_evolution'] = fig\n",
    "        return best_model, best_metrics, study_dt, visualizations\n",
    "\n",
    "    optuna_studies = []\n",
    "    outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    best_f1 = -np.inf\n",
    "    best_model = None\n",
    "    best_metrics = {}\n",
    "\n",
    "    for fold, (train_idx, test_idx) in tqdm(enumerate(outer_cv.split(X, y), 1), total=outer_cv.get_n_splits(), desc=\"Training folds\"):\n",
    "        X_train_fold, X_test_fold = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train_fold, y_test_fold = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "            X_train_fold, y_train_fold, test_size=0.2, stratify=y_train_fold, random_state=SEED\n",
    "        )\n",
    "\n",
    "        start_time = time.time()\n",
    "        best_params, study = run_inner_optuna(X_tr, y_tr, numeric_features, categorical_features)\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        optuna_studies.append(study)\n",
    "\n",
    "        model = RNNWrapper(best_params, numeric_features, categorical_features)\n",
    "        model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)])\n",
    "\n",
    "        f1_train, acc_train, loss_train, auc_train, report_train, cm_train, y_proba_train, y_train_pred = evaluate_model(model, X_tr, y_tr)\n",
    "        f1_val, acc_val, loss_val, auc_val, report_val, cm_val, y_proba_val, y_val_pred = evaluate_model(model, X_val, y_val)\n",
    "        f1_test, acc_test, loss_test, auc_test, report_test, cm_test, y_proba_test, y_test_pred = evaluate_model(model, X_test_fold, y_test_fold)\n",
    "\n",
    "        fold_metrics = {\n",
    "            \"fold\": fold,\n",
    "            \"f1_train\": f1_train,\n",
    "            \"accuracy_train\": acc_train,\n",
    "            \"log_loss_train\": loss_train,\n",
    "            \"auc_train\": auc_train,\n",
    "            \"f1_val\": f1_val,\n",
    "            \"accuracy_val\": acc_val,\n",
    "            \"log_loss_val\": loss_val,\n",
    "            \"auc_val\": auc_val,\n",
    "            \"f1_test\": f1_test,\n",
    "            \"accuracy_test\": acc_test,\n",
    "            \"log_loss_test\": loss_test,\n",
    "            \"auc_test\": auc_test,\n",
    "            \"optuna_time\": elapsed\n",
    "        }\n",
    "        all_folds_metrics.append(fold_metrics)\n",
    "\n",
    "        if f1_test > best_f1:\n",
    "            best_f1 = f1_test\n",
    "            best_model = model\n",
    "            best_metrics = {\n",
    "                **fold_metrics,\n",
    "                \"params\": best_params,\n",
    "                \"labels\": np.unique(y),\n",
    "                \"best_fold\": fold - 1,\n",
    "                \"X_train_fold\": X_tr,\n",
    "                \"y_train_fold\": y_tr,\n",
    "                \"y_train_pred\": y_train_pred,\n",
    "                \"y_train_true\": y_tr,\n",
    "                \"y_proba_train\": y_proba_train,\n",
    "                \"classification_report_train\": report_train,\n",
    "                \"confusion_matrix_train\": cm_train,\n",
    "                \"y_val_true\": y_val,\n",
    "                \"y_val_pred\": y_val_pred,\n",
    "                \"y_proba_val\": y_proba_val,\n",
    "                \"classification_report_val\": report_val,\n",
    "                \"confusion_matrix_val\": cm_val,\n",
    "                \"y_test_fold\": y_test_fold,\n",
    "                \"y_test_pred\": y_test_pred,\n",
    "                \"y_proba_test\": y_proba_test,\n",
    "                \"classification_report_test\": report_test,\n",
    "                \"confusion_matrix_test\": cm_test\n",
    "            }\n",
    "\n",
    "    joblib.dump(best_model, model_filename)\n",
    "    joblib.dump({\"studies\": optuna_studies}, study_filename)\n",
    "    joblib.dump(best_metrics, metrics_filename)\n",
    "    df_folds = save_metrics_folds(all_folds_metrics, fold_metrics_filename)\n",
    "\n",
    "    study_dt = optuna_studies[best_metrics['best_fold']]\n",
    "    visualizations['optimization_history'] = vis.plot_optimization_history(study_dt)\n",
    "    visualizations['parallel_coordinate'] = vis.plot_parallel_coordinate(study_dt)\n",
    "    visualizations['param_importances'] = vis.plot_param_importances(study_dt)\n",
    "\n",
    "    f1_scores = [t.value for t in study_dt.trials]\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=list(range(len(f1_scores))), y=f1_scores, mode='lines+markers', name='F1-score'))\n",
    "    fig.update_layout(title='F1-Score Evolution During Optuna Optimization', xaxis_title='Trial', yaxis_title='F1-Score', template='plotly_dark')\n",
    "    visualizations['f1_score_evolution'] = fig\n",
    "\n",
    "\n",
    "    return best_model, best_metrics, study_dt, visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, metrics, study, visualizations = nested_cv(X, y, numeric_features, categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics['classification_report_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(metrics['confusion_matrix_train'], metrics['labels'], \"Train Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics['classification_report_val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(metrics['confusion_matrix_val'], metrics['labels'], \"Validation Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics['classification_report_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(metrics['confusion_matrix_test'], metrics['labels'], \"Test Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curves_mlp(model):\n",
    "    \"\"\"\n",
    "    Plots training and validation loss curves for Keras MLP model.\n",
    "    \"\"\"\n",
    "    if not hasattr(model, \"history\") or model.history is None:\n",
    "        raise ValueError(\"No training history available.\")\n",
    "\n",
    "    history = model.history.history  # Dict with loss, val_loss, etc.\n",
    "    \n",
    "    if \"loss\" in history:\n",
    "        plt.plot(history[\"loss\"], label=\"Train Loss\")\n",
    "    if \"val_loss\" in history:\n",
    "        plt.plot(history[\"val_loss\"], label=\"Val Loss\")\n",
    "\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training and Validation Loss - MLP\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves_mlp(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_all_metrics(history_dict):\n",
    "    available_metrics = [m for m in history_dict.keys() if not m.startswith('val_')]\n",
    "    \n",
    "    for metric in available_metrics:\n",
    "        val_metric = f'val_{metric}'\n",
    "        if val_metric in history_dict:\n",
    "            plt.figure(figsize=(6, 4))  # más pequeño\n",
    "            plt.plot(history_dict[metric], label=f'Train {metric}')\n",
    "            plt.plot(history_dict[val_metric], label=f'Val {metric}')\n",
    "            plt.title(f'{metric.capitalize()} over Epochs')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel(metric.capitalize())\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "plot_all_metrics(model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construir explainer\n",
    "explainer, X_transformed = get_lime_explainer(model, metrics['X_train_fold'], metrics['y_train_fold'])\n",
    "\n",
    "# Seleccionar instancia\n",
    "instance = X_transformed[10]\n",
    "\n",
    "# Función de predicción para LIME (TF espera batch de entrada)\n",
    "def predict_fn(x):\n",
    "    x_reshaped = x.reshape((x.shape[0], 1, x.shape[1]))  # LSTM espera (samples, timesteps, features)\n",
    "    return model.model.predict(x_reshaped)\n",
    "\n",
    "# Explicación\n",
    "exp = explainer.explain_instance(instance, predict_fn=predict_fn, labels=[0, 1, 2, 3, 4])\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(exp.as_html(show_table=True)))\n",
    "\n",
    "# Mostrar explicaciones para cada clase\n",
    "for class_label in exp.available_labels():\n",
    "    print(f\"--- Explanation for class {class_label} ---\")\n",
    "    fig = exp.as_pyplot_figure(label=class_label)\n",
    "    plt.title(f\"LIME - Class {class_label}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizations.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizations['optimization_history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizations['parallel_coordinate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizations['f1_score_evolution']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizations['param_importances']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reporte.save(metrics, model_name=\"RNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = reporte.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"Model == 'RNN' and Type == 'train' and Class != 'global'\").iloc[:, 0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"Model == 'RNN' and Type == 'val' and Class != 'global'\").iloc[:, 0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"Model == 'RNN' and Type == 'test' and Class != 'global'\").iloc[:, 0:6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"Model == 'RNN' and auc != '-'\")[[\"Model\", \"Type\", \"accuracy\", \"log_loss\", \"auc\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"Model == 'RNN' and Type == 'val' and Class\t== 'global'\")[[\"Model\", \"Type\", \"accuracy\", \"log_loss\", \"auc\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_multiclass(y_true, y_proba, class_labels, title=\"AUC-ROC Curve (Multiclass)\"):\n",
    "    # Binarize true labels\n",
    "    y_bin = label_binarize(y_true, classes=class_labels)\n",
    "    n_classes = len(class_labels)\n",
    "\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], y_proba[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for i in range(n_classes):\n",
    "        plt.plot(fpr[i], tpr[i],\n",
    "                 label=f\"Class {class_labels[i]} (AUC = {roc_auc[i]:.3f})\")\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", label=\"Random\")\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_multiclass(\n",
    "    y_true=metrics[\"y_test_fold\"],\n",
    "    y_proba=metrics[\"y_proba_test\"],\n",
    "    class_labels=metrics[\"labels\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.model.layers:\n",
    "    print(f\"Layer: {layer.name}\")\n",
    "    weights = layer.get_weights()\n",
    "    print(f\" - Num weight arrays: {len(weights)}\")\n",
    "    for i, w in enumerate(weights):\n",
    "        print(f\"   Shape[{i}]: {w.shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TimesSeries",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
