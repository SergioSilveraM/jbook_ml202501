{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Random Forest***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "import optuna.visualization as vis\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, label_binarize, OrdinalEncoder\n",
    "from sklearn.metrics import (\n",
    "    f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    accuracy_score, log_loss, roc_auc_score, roc_curve, auc\n",
    ")\n",
    "import lime.lime_tabular\n",
    "import time\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from reporte_metricas import ReporteMetricas\n",
    "reporte = ReporteMetricas()\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# LOAD DATA\n",
    "# =======================\n",
    "file_path = \"../Saber_pro_sampled_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "#df = df.head(1000)\n",
    "X = df.drop(columns=[\"MOD_INGLES_DESEM\"])\n",
    "y = df[\"MOD_INGLES_DESEM\"]\n",
    "\n",
    "categorical_features = X.select_dtypes(include='object').columns.tolist()\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = pd.Series(le.fit_transform(y), index=y.index)\n",
    "\n",
    "class_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(\"Class mapping:\", class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = \"../Models/best_rf_model.pkl\"\n",
    "study_filename = \"../Study/best_rf_model.pkl\"\n",
    "metrics_filename = \"../Metrics/best_rf_metrics.pkl\"\n",
    "fold_metrics_filename = \"../Metrics/best_rf_model.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# PIPELINE BUILDER\n",
    "# =======================\n",
    "\n",
    "# Explicitly ordered categorical columns\n",
    "ORDINAL_COLUMNS = {\n",
    "    'FAMI_ESTRATOVIVIENDA': [\n",
    "        'Sin Estrato',\n",
    "        'Estrato 1',\n",
    "        'Estrato 2',\n",
    "        'Estrato 3',\n",
    "        'Estrato 4',\n",
    "        'Estrato 5',\n",
    "        'Estrato 6'\n",
    "    ],\n",
    "\n",
    "    'FAMI_EDUCACIONPADRE': [\n",
    "        'Ninguno',\n",
    "        'Primaria incompleta',\n",
    "        'Primaria completa',\n",
    "        'Secundaria (Bachillerato) incompleta',\n",
    "        'Secundaria (Bachillerato) completa',\n",
    "        'T茅cnica o tecnol贸gica incompleta',\n",
    "        'T茅cnica o tecnol贸gica completa',\n",
    "        'Educaci贸n profesional incompleta',\n",
    "        'Educaci贸n profesional completa',\n",
    "        'Postgrado',\n",
    "        'No sabe',\n",
    "        'No Aplica'\n",
    "    ],\n",
    "\n",
    "    'FAMI_EDUCACIONMADRE': [\n",
    "        'Ninguno',\n",
    "        'Primaria incompleta',\n",
    "        'Primaria completa',\n",
    "        'Secundaria (Bachillerato) incompleta',\n",
    "        'Secundaria (Bachillerato) completa',\n",
    "        'T茅cnica o tecnol贸gica incompleta',\n",
    "        'T茅cnica o tecnol贸gica completa',\n",
    "        'Educaci贸n profesional incompleta',\n",
    "        'Educaci贸n profesional completa',\n",
    "        'Postgrado',\n",
    "        'No sabe',\n",
    "        'No Aplica'\n",
    "    ],\n",
    "\n",
    "    'ESTU_HORASSEMANATRABAJA': [\n",
    "        '0',\n",
    "        'Menos de 10 horas',\n",
    "        'Entre 11 y 20 horas',\n",
    "        'Entre 21 y 30 horas',\n",
    "        'M谩s de 30 horas'\n",
    "    ],\n",
    "\n",
    "    'ESTU_VALORMATRICULAUNIVERSIDAD': [\n",
    "        \"No pag贸 matr铆cula\",\n",
    "        \"Menos de 500 mil\",\n",
    "        \"Entre 500 mil y menos de 1 mill贸n\",\n",
    "        \"Entre 1 mill贸n y menos de 2.5 millones\",\n",
    "        \"Entre 2.5 millones y menos de 4 millones\",\n",
    "        \"Entre 4 millones y menos de 5.5 millones\",\n",
    "        \"Entre 5.5 millones y menos de 7 millones\",\n",
    "        \"M谩s de 7 millones\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "def build_pipeline(params: dict, X: pd.DataFrame = None) -> Pipeline:\n",
    "    if X is None:\n",
    "        raise ValueError(\"X must be provided to build the pipeline dynamically.\")\n",
    "\n",
    "    # Identify column types\n",
    "    categorical_features = X.select_dtypes(include='object').columns.tolist()\n",
    "    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "    # Separate manually defined ordinal vs nominal categorical variables\n",
    "    ordinal_cols = [col for col in ORDINAL_COLUMNS if col in X.columns]\n",
    "    nominal_cols = [col for col in categorical_features if col not in ordinal_cols]\n",
    "\n",
    "    transformers = [\n",
    "        ('num', SimpleImputer(strategy='median'), numeric_features)\n",
    "    ]\n",
    "\n",
    "    if nominal_cols:\n",
    "        transformers.append(('ohe_cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='Sin Dato')),\n",
    "            ('ohe', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), nominal_cols))\n",
    "\n",
    "    if ordinal_cols:\n",
    "        for col in ordinal_cols:\n",
    "            categories = [ORDINAL_COLUMNS[col]] if ORDINAL_COLUMNS[col] is not None else 'auto'\n",
    "            transformers.append((f'ord_{col}', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='constant', fill_value='Sin Dato')),\n",
    "                ('ord', OrdinalEncoder(categories=categories if categories != 'auto' else 'auto',\n",
    "                                       handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "            ]), [col]))\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers,\n",
    "        remainder='drop',\n",
    "        verbose_feature_names_out=False\n",
    "    )\n",
    "    \n",
    "    model = RandomForestClassifier(**params, random_state=SEED, class_weight=\"balanced\", n_jobs=8)\n",
    "    return Pipeline([('preprocessor', preprocessor), ('classifier', model)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# MODEL EVALUATION\n",
    "# ============================\n",
    "def evaluate_model(model, X_data, y_data):\n",
    "    y_pred = model.predict(X_data)\n",
    "    y_proba = model.predict_proba(X_data)\n",
    "    f1 = f1_score(y_data, y_pred, average='weighted')\n",
    "    acc = accuracy_score(y_data, y_pred)\n",
    "    loss = log_loss(y_data, y_proba)\n",
    "    auc = roc_auc_score(y_data, y_proba, multi_class='ovr', average='weighted')\n",
    "    report = classification_report(y_data, y_pred)\n",
    "    cm = confusion_matrix(y_data, y_pred)\n",
    "    return f1, acc, loss, auc, report, cm, y_proba, y_pred\n",
    "\n",
    "# ============================\n",
    "# CONFUSION MATRIX PLOTTER\n",
    "# ============================\n",
    "def plot_confusion_matrix(cm, labels, title):\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
    "    disp.ax_.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# HYPERPARAMETER TUNING\n",
    "# =======================\n",
    "def run_inner_optuna(X_inner: pd.DataFrame, y_inner: pd.Series, n_trials: int = 50) -> tuple:\n",
    "    \"\"\"\n",
    "    Inner CV loop using Optuna to optimize DecisionTree hyperparameters.\n",
    "    \"\"\"\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500),  \n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 5, 15), \n",
    "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20), \n",
    "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10), \n",
    "            \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\"]), \n",
    "            \"criterion\": trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"]),\n",
    "            \"bootstrap\": trial.suggest_categorical(\"bootstrap\", [True, False]),\n",
    "            \"ccp_alpha\": trial.suggest_float(\"ccp_alpha\", 1e-6, 1e-2, log=True), \n",
    "            \"max_leaf_nodes\": trial.suggest_int(\"max_leaf_nodes\", 30, 150), \n",
    "        }\n",
    "        pipeline = build_pipeline(params, X_inner)\n",
    "        skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "        scores = []\n",
    "        for train_idx, val_idx in skf.split(X_inner, y_inner):\n",
    "            X_t, X_v = X_inner.iloc[train_idx], X_inner.iloc[val_idx]\n",
    "            y_t, y_v = y_inner.iloc[train_idx], y_inner.iloc[val_idx]\n",
    "            pipeline.fit(X_t, y_t)\n",
    "            y_pred = pipeline.predict(X_v)\n",
    "            scores.append(f1_score(y_v, y_pred, average='weighted'))\n",
    "        return np.mean(scores)\n",
    "\n",
    "    pruner = MedianPruner()\n",
    "    study = optuna.create_study(direction=\"maximize\", pruner=pruner)\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    return study.best_params, study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# FOLD METRIC SAVER\n",
    "# =======================\n",
    "def save_metrics_folds(folds_metrics: list, filename: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Saves per-fold metrics + final rows for mean and std in a single CSV.\n",
    "    Returns full DataFrame (raw + summary).\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(folds_metrics)\n",
    "\n",
    "    metric_cols = df.columns.drop('fold') if 'fold' in df.columns else df.columns\n",
    "    mean_row = df[metric_cols].mean().to_dict()\n",
    "    std_row = df[metric_cols].std().to_dict()\n",
    "\n",
    "    mean_row['fold'] = 'mean'\n",
    "    std_row['fold'] = 'std'\n",
    "\n",
    "    df_final = pd.concat([df, pd.DataFrame([mean_row, std_row])], ignore_index=True)\n",
    "\n",
    "    df_final.to_csv(filename, index=False)\n",
    "    print(f\" Fold metrics + summary saved to: {filename}\")\n",
    "    return df_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# LIME EXPLAINER BUILDER\n",
    "# =======================\n",
    "def get_lime_explainer(model_pipeline: Pipeline, X_train_raw: pd.DataFrame, y_train_raw: pd.Series):\n",
    "    \"\"\"\n",
    "    Constructs LIME explainer using preprocessed training data.\n",
    "    \"\"\"\n",
    "    X_transformed = model_pipeline.named_steps['preprocessor'].transform(X_train_raw)\n",
    "    feature_names = model_pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
    "    class_names = np.unique(y_train_raw).astype(str)\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "        training_data=X_transformed,\n",
    "        feature_names=feature_names,\n",
    "        class_names=class_names,\n",
    "        mode='classification'\n",
    "    )\n",
    "    return explainer, X_transformed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# NESTED CV LOOP\n",
    "# ============================\n",
    "def nested_cv(X: pd.DataFrame, y: pd.Series) -> tuple:\n",
    "    \"\"\"\n",
    "    Executes nested CV with full metric tracking (train/val/test), including classification reports,\n",
    "    confusion matrices and predict_proba for LIME. Also includes Optuna visualizations.\n",
    "    \"\"\"\n",
    "    visualizations = {}\n",
    "    all_folds_metrics = []\n",
    "\n",
    "    if os.path.exists(model_filename) and os.path.exists(metrics_filename):\n",
    "        best_model = joblib.load(model_filename)\n",
    "        best_metrics = joblib.load(metrics_filename)\n",
    "        if \"best_fold\" in best_metrics:\n",
    "            study_dt = joblib.load(study_filename)['studies'][best_metrics['best_fold']]\n",
    "            print(\"Loaded best model and metrics from disk.\")\n",
    "        else:\n",
    "            study_dt = None\n",
    "        if study_dt:\n",
    "            visualizations['optimization_history'] = vis.plot_optimization_history(study_dt)\n",
    "            visualizations['parallel_coordinate'] = vis.plot_parallel_coordinate(study_dt)\n",
    "            visualizations['param_importances'] = vis.plot_param_importances(study_dt)\n",
    "            f1_scores = [t.value for t in study_dt.trials]\n",
    "            fig = go.Figure()\n",
    "            fig.add_trace(go.Scatter(x=list(range(len(f1_scores))), y=f1_scores, mode='lines+markers', name='F1-score'))\n",
    "            fig.update_layout(title='F1-Score Evolution During Optuna Optimization', xaxis_title='Trial', yaxis_title='F1-Score', template='plotly_dark')\n",
    "            visualizations['f1_score_evolution'] = fig\n",
    "        return best_model, best_metrics, study_dt, visualizations\n",
    "\n",
    "    optuna_studies = []\n",
    "    outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    best_f1 = -np.inf\n",
    "    best_model = None\n",
    "    best_metrics = {}\n",
    "\n",
    "    for fold, (train_idx, test_idx) in tqdm(\n",
    "        enumerate(outer_cv.split(X, y), 1),\n",
    "        total=outer_cv.get_n_splits(),\n",
    "        desc=\"Training folds\"\n",
    "    ):\n",
    "        X_train_fold, X_test_fold = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train_fold, y_test_fold = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        # Extra validation split for reporting\n",
    "        X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "            X_train_fold, y_train_fold,\n",
    "            test_size=0.2,\n",
    "            stratify=y_train_fold,\n",
    "            random_state=SEED\n",
    "        )\n",
    "\n",
    "        start_time = time.time()\n",
    "        best_params, study = run_inner_optuna(X_tr, y_tr)\n",
    "        optuna_time = time.time() - start_time\n",
    "        optuna_studies.append(study)\n",
    "        model = build_pipeline(best_params, X_tr)\n",
    "        model.fit(X_tr, y_tr)\n",
    "\n",
    "        # FULL train evaluation\n",
    "        f1_train, acc_train, loss_train, auc_train, report_train, cm_train, y_proba_train, y_train_pred = evaluate_model(model, X_tr, y_tr)\n",
    "\n",
    "        # Validation evaluation\n",
    "        f1_val, acc_val, loss_val, auc_val, report_val, cm_val, y_proba_val, y_val_pred = evaluate_model(model, X_val, y_val)\n",
    "\n",
    "        # Outer test evaluation\n",
    "        f1_test, acc_test, loss_test, auc_test, report_test, cm_test, y_proba_test, y_test_pred = evaluate_model(model, X_test_fold, y_test_fold)\n",
    "\n",
    "        fold_metrics = {\n",
    "            \"fold\": fold,\n",
    "            \"f1_train\": f1_train,\n",
    "            \"accuracy_train\": acc_train,\n",
    "            \"log_loss_train\": loss_train,\n",
    "            \"auc_train\": auc_train,\n",
    "            \"f1_val\": f1_val,\n",
    "            \"accuracy_val\": acc_val,\n",
    "            \"log_loss_val\": loss_val,\n",
    "            \"auc_val\": auc_val,\n",
    "            \"f1_test\": f1_test,\n",
    "            \"accuracy_test\": acc_test,\n",
    "            \"log_loss_test\": loss_test,\n",
    "            \"auc_test\": auc_test,\n",
    "            \"optuna_time\": optuna_time\n",
    "        }\n",
    "        all_folds_metrics.append(fold_metrics)\n",
    "\n",
    "        if f1_test > best_f1:\n",
    "            best_f1 = f1_test\n",
    "            best_model = model\n",
    "            best_metrics = {\n",
    "                **fold_metrics,\n",
    "                \"params\": best_params,\n",
    "                \"labels\": model.named_steps['classifier'].classes_,\n",
    "                \"best_fold\": fold - 1,\n",
    "                \"X_train_fold\": X_tr,\n",
    "                \"y_train_fold\": y_tr,\n",
    "                \"y_train_pred\": y_train_pred,\n",
    "                \"y_train_true\": y_tr,\n",
    "                \"y_proba_train\": y_proba_train,\n",
    "                \"classification_report_train\": report_train,\n",
    "                \"confusion_matrix_train\": cm_train,\n",
    "                \"y_val_true\": y_val,\n",
    "                \"y_val_pred\": y_val_pred,\n",
    "                \"y_proba_val\": y_proba_val,\n",
    "                \"classification_report_val\": report_val,\n",
    "                \"confusion_matrix_val\": cm_val,\n",
    "                \"y_test_fold\": y_test_fold,\n",
    "                \"y_test_pred\": y_test_pred,\n",
    "                \"y_proba_test\": y_proba_test,\n",
    "                \"classification_report_test\": report_test,\n",
    "                \"confusion_matrix_test\": cm_test\n",
    "            }\n",
    "\n",
    "    # Visualizations for best study\n",
    "    study_dt = optuna_studies[best_metrics['best_fold']]\n",
    "    visualizations['optimization_history'] = vis.plot_optimization_history(study_dt)\n",
    "    visualizations['parallel_coordinate'] = vis.plot_parallel_coordinate(study_dt)\n",
    "    visualizations['param_importances'] = vis.plot_param_importances(study_dt)\n",
    "    f1_scores = [t.value for t in study_dt.trials]\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=list(range(len(f1_scores))), y=f1_scores, mode='lines+markers', name='F1-score'))\n",
    "    fig.update_layout(title='F1-Score Evolution During Optuna Optimization', xaxis_title='Trial', yaxis_title='F1-Score', template='plotly_dark')\n",
    "    visualizations['f1_score_evolution'] = fig\n",
    "\n",
    "    return best_model, best_metrics, study_dt, visualizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pipeline\n",
    "model, metrics, study, visualizations = nested_cv(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics['classification_report_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(metrics['confusion_matrix_train'], metrics['labels'], \"Train Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics['classification_report_val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(metrics['confusion_matrix_val'], metrics['labels'], \"Validation Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics['classification_report_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(metrics['confusion_matrix_test'], metrics['labels'], \"Test Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get LIME explainer and transformed data\n",
    "explainer, X_transformed = get_lime_explainer(model, metrics['X_train_fold'], metrics['y_train_fold'])\n",
    "\n",
    "# Extract raw classifier (without preprocessing)\n",
    "clf = model.named_steps['classifier']\n",
    "\n",
    "# Prediction function for LIME\n",
    "predict_fn = lambda x: clf.predict_proba(x)\n",
    "\n",
    "# Select transformed instance\n",
    "instance = X_transformed[10]\n",
    "\n",
    "# Explain prediction for all classes\n",
    "exp = explainer.explain_instance(instance, predict_fn=predict_fn, labels=[0, 1, 2])\n",
    "\n",
    "# Show explanation summary\n",
    "exp.show_in_notebook(show_table=True)\n",
    "\n",
    "# Plot per-class explanations\n",
    "for class_label in exp.available_labels():\n",
    "    print(f\"--- Explanation for class {class_label} ---\")\n",
    "    fig = exp.as_pyplot_figure(label=class_label)\n",
    "    plt.title(f\"LIME - Class {class_label}\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizations.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizations['optimization_history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizations['parallel_coordinate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizations['f1_score_evolution']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizations['param_importances']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance Plot\n",
    "importances = model.named_steps['classifier'].feature_importances_\n",
    "feature_names = model.named_steps['preprocessor'].get_feature_names_out()\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False).head(20)\n",
    "\n",
    "fig = px.bar(importance_df,\n",
    "             x='Feature',\n",
    "             y='Importance',\n",
    "             title='Top 20 Feature Importance of the Decision Tree Model',\n",
    "             labels={'Feature': 'Feature Name', 'Importance': 'Importance Score'},\n",
    "             color='Importance',\n",
    "             color_continuous_scale='Viridis',\n",
    "             text='Importance')\n",
    "fig.update_layout(xaxis_tickangle=45)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reporte.save(metrics, model_name=\"RF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = reporte.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"Model == 'RF' and Type == 'train' and Class != 'global'\").iloc[:, 0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"Model == 'RF' and Type == 'val' and Class != 'global'\").iloc[:, 0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"Model == 'RF' and Type == 'test' and Class != 'global'\").iloc[:, 0:6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"Model == 'RF' and auc != '-'\")[[\"Model\", \"Type\", \"accuracy\", \"log_loss\", \"auc\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"Model == 'RF' and Type == 'val' and Class\t== 'global'\")[[\"Model\", \"Type\", \"accuracy\", \"log_loss\", \"auc\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_multiclass(y_true, y_proba, class_labels, title=\"AUC-ROC Curve (Multiclass)\"):\n",
    "    # Binarize true labels\n",
    "    y_bin = label_binarize(y_true, classes=class_labels)\n",
    "    n_classes = len(class_labels)\n",
    "\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], y_proba[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for i in range(n_classes):\n",
    "        plt.plot(fpr[i], tpr[i],\n",
    "                 label=f\"Class {class_labels[i]} (AUC = {roc_auc[i]:.3f})\")\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", label=\"Random\")\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_multiclass(\n",
    "    y_true=metrics[\"y_test_fold\"],\n",
    "    y_proba=metrics[\"y_proba_test\"],\n",
    "    class_labels=metrics[\"labels\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.named_steps[\"classifier\"].get_params()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
