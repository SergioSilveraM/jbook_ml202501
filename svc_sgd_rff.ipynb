{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***SVC RFF***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, label_binarize\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    accuracy_score, log_loss, roc_auc_score, roc_curve, auc\n",
    ")\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "import optuna.visualization as vis\n",
    "import plotly.graph_objects as go\n",
    "import lime.lime_tabular\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "\n",
    "\n",
    "from reporte_metricas import ReporteMetricas\n",
    "reporte = ReporteMetricas()\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =======================\n",
    "# LOAD DATA\n",
    "# =======================\n",
    "file_path = \"../Saber_pro_sampled_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "#df = df.head(3000)\n",
    "X = df.drop(columns=[\"MOD_INGLES_DESEM\"])\n",
    "y = df[\"MOD_INGLES_DESEM\"]\n",
    "\n",
    "categorical_features = X.select_dtypes(include='object').columns.tolist()\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = pd.Series(le.fit_transform(y), index=y.index)\n",
    "\n",
    "class_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(\"Class mapping:\", class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# FILE PATHS\n",
    "# ============================\n",
    "model_filename = \"../Models/best_svc_model_rff.pkl\"\n",
    "study_filename = \"../Study/optuna_study_SVC_nested_rff.pkl\"\n",
    "metrics_filename = \"../Models/best_svc_metrics_rff.pkl\"\n",
    "fold_metrics_filename = \"../Metrics/svc_folds_summary_rff.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# MODEL PIPELINE BUILDER\n",
    "# ============================\n",
    "def build_pipeline(params: dict, numeric_features: list, categorical_features: list):\n",
    "    preprocessor = ColumnTransformer([\n",
    "        (\"num\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler())\n",
    "        ]), numeric_features),\n",
    "        (\"cat\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"Sin Dato\")),\n",
    "            (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "        ]), categorical_features)\n",
    "    ],\n",
    "    verbose_feature_names_out=False,\n",
    "    force_int_remainder_cols=False\n",
    "    )\n",
    "\n",
    "    rff = RBFSampler(\n",
    "        gamma=params.pop(\"gamma_rff\", 1.0),\n",
    "        random_state=SEED\n",
    "    )\n",
    "\n",
    "    sgd = SGDClassifier(\n",
    "        loss=\"hinge\",\n",
    "        random_state=SEED,\n",
    "        class_weight='balanced',\n",
    "        **params\n",
    "    )\n",
    "\n",
    "    model = Pipeline([\n",
    "        (\"rff\", rff),\n",
    "        (\"sgd\", sgd)\n",
    "    ])\n",
    "\n",
    "    return Pipeline(steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"classifier\", model)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# MODEL EVALUATION\n",
    "# ============================\n",
    "def evaluate_model_sgd(model, X_data, y_data):\n",
    "    y_pred = model.predict(X_data)\n",
    "    #y_proba = model.predict_proba(X_data)\n",
    "    f1 = f1_score(y_data, y_pred, average='weighted')\n",
    "    acc = accuracy_score(y_data, y_pred)\n",
    "    #loss = log_loss(y_data, y_proba)\n",
    "    #auc = roc_auc_score(y_data, y_proba, multi_class='ovr', average='weighted')\n",
    "    report = classification_report(y_data, y_pred)\n",
    "    cm = confusion_matrix(y_data, y_pred)\n",
    "    return f1, acc, report, cm, y_pred\n",
    "\n",
    "# ============================\n",
    "# CONFUSION MATRIX PLOTTER\n",
    "# ============================\n",
    "def plot_confusion_matrix(cm, labels, title):\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
    "    disp.ax_.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# OPTUNA INNER CV OPTIMIZATION\n",
    "# ============================\n",
    "def run_inner_optuna(X_inner, y_inner, numeric_features, categorical_features, n_trials=50):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"alpha\": trial.suggest_float(\"alpha\", 1e-6, 1e-2, log=True),\n",
    "            \"penalty\": trial.suggest_categorical(\"penalty\", [\"l2\"]),\n",
    "            \"gamma_rff\": trial.suggest_float(\"gamma_rff\", 0.01, 1.0, log=True),\n",
    "            \"max_iter\": 1000,\n",
    "            \"tol\": 1e-3\n",
    "        }\n",
    "\n",
    "        model = build_pipeline(params, numeric_features, categorical_features)\n",
    "        skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "        scores = []\n",
    "\n",
    "        for train_idx, val_idx in skf.split(X_inner, y_inner):\n",
    "            X_t, X_v = X_inner.iloc[train_idx], X_inner.iloc[val_idx]\n",
    "            y_t, y_v = y_inner.iloc[train_idx], y_inner.iloc[val_idx]\n",
    "            model.fit(X_t, y_t)\n",
    "            y_pred = model.predict(X_v)\n",
    "            scores.append(f1_score(y_v, y_pred, average='weighted'))\n",
    "\n",
    "        return np.mean(scores)\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\", pruner=MedianPruner())\n",
    "    study.optimize(objective, n_trials=n_trials, n_jobs=5)\n",
    "    return study.best_params, study\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# SAVE FOLD METRICS\n",
    "# ============================\n",
    "def save_metrics_folds(folds_metrics: list, filename: str):\n",
    "    df = pd.DataFrame(folds_metrics)\n",
    "    resumen = df.describe().T[['mean', 'std']].reset_index()\n",
    "    resumen.rename(columns={'index': 'metric'}, inplace=True)\n",
    "    df_full = pd.concat([df, resumen], axis=0)\n",
    "    df_full.to_csv(filename, index=False)\n",
    "    return df, resumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# LIME EXPLAINER BUILDER\n",
    "# ============================\n",
    "def get_lime_explainer(model_pipeline, X_train_raw, y_train_raw):\n",
    "    X_transformed = model_pipeline.named_steps['preprocessor'].transform(X_train_raw)\n",
    "    feature_names = model_pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
    "    class_names = np.unique(y_train_raw).astype(str)\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "        training_data=X_transformed,\n",
    "        feature_names=feature_names,\n",
    "        class_names=class_names,\n",
    "        mode='classification'\n",
    "    )\n",
    "    return explainer, X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# NESTED CV LOOP\n",
    "# ============================\n",
    "def nested_cv(X, y, numeric_features, categorical_features):\n",
    "    visualizations = {}\n",
    "    all_folds_metrics = []\n",
    "\n",
    "    if os.path.exists(model_filename) and os.path.exists(metrics_filename):\n",
    "        best_model = joblib.load(model_filename)\n",
    "        best_metrics = joblib.load(metrics_filename)\n",
    "        if \"best_fold\" in best_metrics:\n",
    "            study_dt = joblib.load(study_filename)['studies'][best_metrics['best_fold']]\n",
    "        else:\n",
    "            study_dt = None\n",
    "        if study_dt:\n",
    "            visualizations['optimization_history'] = vis.plot_optimization_history(study_dt)\n",
    "            visualizations['parallel_coordinate'] = vis.plot_parallel_coordinate(study_dt)\n",
    "            visualizations['param_importances'] = vis.plot_param_importances(study_dt)\n",
    "            f1_scores = [t.value for t in study_dt.trials]\n",
    "            fig = go.Figure()\n",
    "            fig.add_trace(go.Scatter(x=list(range(len(f1_scores))), y=f1_scores, mode='lines+markers', name='F1-score'))\n",
    "            fig.update_layout(title='F1-Score Evolution During Optuna Optimization', xaxis_title='Trial', yaxis_title='F1-Score', template='plotly_dark')\n",
    "            visualizations['f1_score_evolution'] = fig\n",
    "        return best_model, best_metrics, study_dt, visualizations\n",
    "\n",
    "    outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    best_model, best_metrics = None, {}\n",
    "    best_f1 = -np.inf\n",
    "    optuna_studies = []\n",
    "    best_fold = 1\n",
    "\n",
    "    for fold, (train_idx, test_idx) in tqdm(\n",
    "        enumerate(outer_cv.split(X, y), 1),\n",
    "        total=outer_cv.get_n_splits(),\n",
    "        desc = \"Training folds\"\n",
    "    ):\n",
    "        \n",
    "        X_train_fold, X_test_fold = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train_fold, y_test_fold = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "                X_train_fold, y_train_fold,\n",
    "                test_size=0.2, stratify=y_train_fold, random_state=SEED)\n",
    "\n",
    "        start_time = time.time()\n",
    "        best_params, study = run_inner_optuna(X_tr, y_tr, numeric_features, categorical_features)\n",
    "        elapsed = time.time() - start_time\n",
    "        optuna_studies.append(study)\n",
    "\n",
    "        model = build_pipeline(best_params, numeric_features, categorical_features)\n",
    "        model.fit(X_tr, y_tr)\n",
    "\n",
    "        f1_train, acc_train, report_train, cm_train, y_train_pred = evaluate_model_sgd(model, X_tr, y_tr)\n",
    "        f1_val, acc_val, report_val, cm_val, y_val_pred = evaluate_model_sgd(model, X_val, y_val)\n",
    "        f1_test, acc_test, report_test, cm_test, y_test_pred = evaluate_model_sgd(model, X_test_fold, y_test_fold)\n",
    "\n",
    "        fold_metrics = {\n",
    "            \"fold\": fold,\n",
    "            \"f1_train\": f1_train,\n",
    "            \"accuracy_train\": acc_train,\n",
    "            \"log_loss_train\": 999,\n",
    "            \"auc_train\": 0,\n",
    "            \"f1_val\": f1_val,\n",
    "            \"accuracy_val\": acc_val,\n",
    "            \"log_loss_val\": 999,\n",
    "            \"auc_val\": 0,\n",
    "            \"f1_test\": f1_test,\n",
    "            \"accuracy_test\": acc_test,\n",
    "            \"log_loss_test\":999,\n",
    "            \"auc_test\": 0,\n",
    "            \"optuna_time\": elapsed\n",
    "        }\n",
    "        all_folds_metrics.append(fold_metrics)\n",
    "\n",
    "        if f1_test > best_f1:\n",
    "            best_f1 = f1_test\n",
    "            best_model = model\n",
    "            best_metrics = {\n",
    "                **fold_metrics,\n",
    "                \"params\": best_params,\n",
    "                \"labels\": np.unique(y),\n",
    "                \"best_fold\": fold - 1,\n",
    "                \"X_train_fold\": X_tr,\n",
    "                \"y_train_fold\": y_tr,\n",
    "                \"y_train_pred\": y_train_pred,\n",
    "                \"y_train_true\": y_tr,\n",
    "                #\"y_proba_train\": y_proba_train,\n",
    "                \"classification_report_train\": report_train,\n",
    "                \"confusion_matrix_train\": cm_train,\n",
    "                \"y_val_true\": y_val,\n",
    "                \"y_val_pred\": y_val_pred,\n",
    "                #\"y_proba_val\": y_proba_val,\n",
    "                \"classification_report_val\": report_val,\n",
    "                \"confusion_matrix_val\": cm_val,\n",
    "                \"y_test_fold\": y_test_fold,\n",
    "                \"y_test_pred\": y_test_pred,\n",
    "                #\"y_proba_test\": y_proba_test,\n",
    "                \"classification_report_test\": report_test,\n",
    "                \"confusion_matrix_test\": cm_test\n",
    "            }\n",
    "\n",
    "    joblib.dump(best_model, model_filename)\n",
    "    joblib.dump({\"studies\": optuna_studies}, study_filename)\n",
    "    joblib.dump(best_metrics, metrics_filename)\n",
    "    save_metrics_folds(all_folds_metrics, fold_metrics_filename)\n",
    "\n",
    "    study_dt = optuna_studies[best_metrics['best_fold']]\n",
    "    visualizations['optimization_history'] = vis.plot_optimization_history(study_dt)\n",
    "    visualizations['parallel_coordinate'] = vis.plot_parallel_coordinate(study_dt)\n",
    "    visualizations['param_importances'] = vis.plot_param_importances(study_dt)\n",
    "\n",
    "    f1_scores = [t.value for t in study_dt.trials]\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=list(range(len(f1_scores))), y=f1_scores, mode='lines+markers', name='F1-score'))\n",
    "    fig.update_layout(title='F1-Score Evolution During Optuna Optimization', xaxis_title='Trial', yaxis_title='F1-Score', template='plotly_dark')\n",
    "    visualizations['f1_score_evolution'] = fig\n",
    "\n",
    "    return best_model, best_metrics, study_dt, visualizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pipeline\n",
    "model, metrics, study, visualizations = nested_cv(X, y, numeric_features, categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics['classification_report_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(metrics['confusion_matrix_train'], metrics['labels'], \"Train Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics['classification_report_val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(metrics['confusion_matrix_val'], metrics['labels'], \"Validation Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics['classification_report_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(metrics['confusion_matrix_test'], metrics['labels'], \"Test Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizations['optimization_history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizations['parallel_coordinate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizations['f1_score_evolution']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizations['param_importances']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reporte.save(metrics, model_name=\"SVC_RFF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = reporte.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"Model == 'SVC_RFF' and Type == 'train' and Class != 'global'\").iloc[:, 0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"Model == 'SVC_RFF' and Type == 'val' and Class != 'global'\").iloc[:, 0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"Model == 'SVC_RFF' and Type == 'test' and Class != 'global'\").iloc[:, 0:6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"Model == 'SVC_RFF' and auc != '-'\")[[\"Model\", \"Type\", \"accuracy\", \"log_loss\", \"auc\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"Model == 'SVC_RFF' and Type == 'val' and Class\t== 'global'\")[[\"Model\", \"Type\", \"accuracy\", \"log_loss\", \"auc\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.named_steps['classifier'].get_params()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
